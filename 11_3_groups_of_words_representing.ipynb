{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 11.3 Two approaches for representing groups of words: Sets and sequences","metadata":{}},{"cell_type":"markdown","source":"How a machine learning model should represent individual words is a relatively uncontroversial question: they’re categorical features (values from a predefined set), and we\nknow how to handle those. They should be encoded as dimensions in a feature space,\nor as category vectors (word vectors in this case). A much more problematic question,\nhowever, is how to encode *the way words are woven into sentences*: word order.\n\nThe problem of order in natural language is an interesting one: unlike the steps of\na timeseries, words in a sentence don’t have a natural, canonical order. Different languages order similar words in very different ways. For instance, the sentence structure\nof English is quite different from that of Japanese. Even within a given language, you\ncan typically say the same thing in different ways by reshuffling the words a bit. Even\nfurther, if you fully randomize the words in a short sentence, you can still largely figure out what it was saying—though in many cases significant ambiguity seems to arise.\nOrder is clearly important, but its relationship to meaning isn’t straightforward.\n\n How to represent word order is the pivotal question from which different kinds of\nNLP architectures spring. The simplest thing you could do is just discard order and\ntreat text as an unordered set of words—this gives you *bag-of-words models*. You could\nalso decide that words should be processed strictly in the order in which they appear,\none at a time, like steps in a timeseries—you could then leverage the recurrent models\nfrom the last chapter. Finally, a hybrid approach is also possible: the Transformer architecture is technically order-agnostic, yet it injects word-position information into\nthe representations it processes, which enables it to simultaneously look at different\nparts of a sentence (unlike RNNs) while still being order-aware. Because they take into\naccount word order, both RNNs and Transformers are called *sequence models*.\n\nHistorically, most early applications of machine learning to NLP just involved\nbag-of-words models. Interest in sequence models only started rising in 2015, with the\nrebirth of recurrent neural networks. Today, both approaches remain relevant. Let’s\nsee how they work, and when to leverage which.\n\nWe’ll demonstrate each approach on a well-known text classification benchmark:\nthe IMDB movie review sentiment-classification dataset. In chapters 4 and 5, you\nworked with a prevectorized version of the IMDB dataset; now, let’s process the raw\nIMDB text data, just like you would do when approaching a new text-classification\nproblem in the real world.","metadata":{}},{"cell_type":"markdown","source":"## 11.3.1 Preparing the IMDB movie reviews data\n\nLet’s start by downloading the dataset from the Stanford page of Andrew Maas and\nuncompressing it:","metadata":{}},{"cell_type":"code","source":"!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n!tar -xf aclImdb_v1.tar.gz","metadata":{"execution":{"iopub.status.busy":"2023-08-09T05:03:43.694190Z","iopub.execute_input":"2023-08-09T05:03:43.694845Z","iopub.status.idle":"2023-08-09T05:04:03.993221Z","shell.execute_reply.started":"2023-08-09T05:03:43.694808Z","shell.execute_reply":"2023-08-09T05:04:03.991578Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"curl: /opt/conda/lib/libcurl.so.4: no version information available (required by curl)\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100 80.2M  100 80.2M    0     0  7556k      0  0:00:10  0:00:10 --:--:-- 15.2M\n","output_type":"stream"}]},{"cell_type":"code","source":"!rm -r aclImdb/train/unsup","metadata":{"execution":{"iopub.status.busy":"2023-08-09T05:07:53.166651Z","iopub.execute_input":"2023-08-09T05:07:53.167244Z","iopub.status.idle":"2023-08-09T05:07:55.810270Z","shell.execute_reply.started":"2023-08-09T05:07:53.167199Z","shell.execute_reply":"2023-08-09T05:07:55.808768Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"You’re left with a directory named aclImdb. The train/pos/ directory contains a set of 12,500 text files, each of which\ncontains the text body of a positive-sentiment movie review to be used as training data.\nThe negative-sentiment reviews live in the “neg” directories. In total, there are 25,000\ntext files for training and another 25,000 for testing.\n\nTake a look at the content of a few of these text files. Whether you’re working with\ntext data or image data, remember to always inspect what your data looks like before\nyou dive into modeling it. It will ground your intuition about what your model is actually doing:","metadata":{}},{"cell_type":"code","source":"!cat aclImdb/train/pos/4077_10.txt","metadata":{"execution":{"iopub.status.busy":"2023-08-09T05:08:00.588358Z","iopub.execute_input":"2023-08-09T05:08:00.588832Z","iopub.status.idle":"2023-08-09T05:08:01.698452Z","shell.execute_reply.started":"2023-08-09T05:08:00.588789Z","shell.execute_reply":"2023-08-09T05:08:01.696991Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"I first saw this back in the early 90s on UK TV, i did like it then but i missed the chance to tape it, many years passed but the film always stuck with me and i lost hope of seeing it TV again, the main thing that stuck with me was the end, the hole castle part really touched me, its easy to watch, has a great story, great music, the list goes on and on, its OK me saying how good it is but everyone will take there own best bits away with them once they have seen it, yes the animation is top notch and beautiful to watch, it does show its age in a very few parts but that has now become part of it beauty, i am so glad it has came out on DVD as it is one of my top 10 films of all time. Buy it or rent it just see it, best viewing is at night alone with drink and food in reach so you don't have to stop the film.<br /><br />Enjoy","output_type":"stream"}]},{"cell_type":"markdown","source":"Next, let’s prepare a validation set by setting apart 20% of the training text files in a\nnew directory, aclImdb/val:","metadata":{}},{"cell_type":"code","source":"import os, pathlib, shutil, random\n\nbase_dir = pathlib.Path(\"aclImdb\")\nval_dir = base_dir / \"val\"\ntrain_dir = base_dir / \"train\"\nfor category in (\"neg\", \"pos\"):\n    os.makedirs(val_dir / category)\n    files = os.listdir(train_dir / category)\n    # Shuffle the list of training files using a seed, to ensure we get the same validation set every time we run the code.\n    random.Random(1337).shuffle(files)\n    # Take 20% of the training files to use for validation.\n    num_val_samples = int(0.2 * len(files))\n    val_files = files[-num_val_samples:]\n    # Move the files to aclImdb/val/neg and aclImdb/val/pos.\n    for fname in val_files:\n        shutil.move(train_dir / category / fname, val_dir / category / fname)","metadata":{"execution":{"iopub.status.busy":"2023-08-09T05:08:05.732530Z","iopub.execute_input":"2023-08-09T05:08:05.733419Z","iopub.status.idle":"2023-08-09T05:08:06.117411Z","shell.execute_reply.started":"2023-08-09T05:08:05.733375Z","shell.execute_reply":"2023-08-09T05:08:06.116057Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"Remember how, in chapter 8, we used the `image_dataset_from_directory` utility to\ncreate a batched `Dataset` of images and their labels for a directory structure? You can\ndo the exact same thing for text files using the `text_dataset_from_directory` utility.\nLet’s create three Dataset objects for training, validation, and testing:","metadata":{}},{"cell_type":"code","source":"from tensorflow import keras","metadata":{"execution":{"iopub.status.busy":"2023-08-09T05:08:22.613682Z","iopub.execute_input":"2023-08-09T05:08:22.614581Z","iopub.status.idle":"2023-08-09T05:08:22.620221Z","shell.execute_reply.started":"2023-08-09T05:08:22.614532Z","shell.execute_reply":"2023-08-09T05:08:22.618793Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"batch_size = 32\n\ntrain_ds = keras.utils.text_dataset_from_directory(\n    \"aclImdb/train\", batch_size=batch_size\n)\nval_ds = keras.utils.text_dataset_from_directory(\n    \"aclImdb/val\", batch_size=batch_size\n)\ntest_ds = keras.utils.text_dataset_from_directory(\n    \"aclImdb/test\", batch_size=batch_size\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-09T05:08:25.862713Z","iopub.execute_input":"2023-08-09T05:08:25.863188Z","iopub.status.idle":"2023-08-09T05:08:30.581127Z","shell.execute_reply.started":"2023-08-09T05:08:25.863147Z","shell.execute_reply":"2023-08-09T05:08:30.580175Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Found 20000 files belonging to 2 classes.\nFound 5000 files belonging to 2 classes.\nFound 25000 files belonging to 2 classes.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"These datasets yield inputs that are TensorFlow `tf.string` tensors and targets that are `int32` tensors encoding the value “0” or “1.”\n\n### Displaying the shapes and dtypes of the first batch","metadata":{}},{"cell_type":"code","source":"for inputs, targets in train_ds:\n    print(\"inputs.shape:\", inputs.shape)\n    print(\"inputs.dtype:\", inputs.dtype)\n    print(\"targets.shape:\", targets.shape)\n    print(\"targets.dtype:\", targets.dtype)\n    print(\"inputs[0]:\", inputs[0])\n    print(\"targets[0]:\", targets[0])\n    break","metadata":{"execution":{"iopub.status.busy":"2023-08-09T05:08:43.630250Z","iopub.execute_input":"2023-08-09T05:08:43.630676Z","iopub.status.idle":"2023-08-09T05:08:43.771869Z","shell.execute_reply.started":"2023-08-09T05:08:43.630641Z","shell.execute_reply":"2023-08-09T05:08:43.770517Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"inputs.shape: (32,)\ninputs.dtype: <dtype: 'string'>\ntargets.shape: (32,)\ntargets.dtype: <dtype: 'int32'>\ninputs[0]: tf.Tensor(b'The Comeback starts off looking promising, with a brutal death scene by a mask wearing killer. The mask itself is pretty cool too, and looks almost identical to the one used in the 1990\\'s slasher film \"Granny\". From then on the film is mostly boring. We get a few more deaths, which again are good, but there\\'s not enough of them. The reason the deaths are so good is because they are frenzied and bloody. The story behind the film is actually rather interesting and would have worked very well had it not been so boring for the most part. <br /><br />I would avoid this unless you\\'re a die-hard collector - there\\'s not enough here to even make it an average slasher flick.', shape=(), dtype=string)\ntargets[0]: tf.Tensor(0, shape=(), dtype=int32)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 11.3.2 Processing words as a set: The bag-of-words approach\n\nThe simplest way to encode a piece of text for processing by a machine learning\nmodel is to discard order and treat it as a set (a “bag”) of tokens. You could either look\nat individual words (unigrams), or try to recover some local order information by\nlooking at groups of consecutive token (N-grams).\n\n**SINGLE WORDS (UNIGRAMS) WITH BINARY ENCODING**\n\nIf you use a bag of single words, the sentence “the cat sat on the mat” becomes\n```\n{\"cat\", \"mat\", \"on\", \"sat\", \"the\"}\n```\nThe main advantage of this encoding is that you can represent an entire text as a single vector, where each entry is a presence indicator for a given word. For instance,\nusing binary encoding (multi-hot), you’d encode a text as a vector with as many\ndimensions as there are words in your vocabulary—with 0s almost everywhere and\nsome 1s for dimensions that encode words present in the text. This is what we did\nwhen we worked with text data in chapters 4 and 5. Let’s try this on our task.\n\nFirst, let’s process our raw text datasets with a `TextVectorization` layer so that\nthey yield multi-hot encoded binary word vectors. Our layer will only look at single\nwords (that is to say, unigrams).\n\n### Preprocessing our datasets with a `TextVectorization` layer\n\nLimit the vocabulary to the 20,000 most frequent words.\nOtherwise we’d be indexing every word in the training data—\npotentially tens of thousands of terms that only occur once or\ntwice and thus aren’t informative. In general, 20,000 is the\nright vocabulary size for text classification.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.layers import TextVectorization\n\ntext_vectorization = TextVectorization(\n    max_tokens=20000,\n    # Encode the output tokens as multi-hot binary vectors.\n    output_mode=\"multi_hot\",\n)\n\n# Prepare a dataset that only yields raw text inputs (no labels).\ntext_only_train_ds = train_ds.map(lambda x, y: x)\n# Use that dataset to index the dataset vocabulary via the adapt() method.\ntext_vectorization.adapt(text_only_train_ds)\n\n# Prepare processed versions of our training, validation, and test dataset.\n# Make sure to specify num_parallel_calls to leverage multiple CPU cores.\nbinary_1gram_train_ds = train_ds.map(\n    lambda x, y: (text_vectorization(x), y),\n    num_parallel_calls=4\n)\nbinary_1gram_val_ds = val_ds.map(\n    lambda x, y: (text_vectorization(x), y),\n    num_parallel_calls=4\n)\nbinary_1gram_test_ds = test_ds.map(\n    lambda x, y: (text_vectorization(x), y),\n    num_parallel_calls=4\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-09T05:08:50.476857Z","iopub.execute_input":"2023-08-09T05:08:50.477778Z","iopub.status.idle":"2023-08-09T05:08:54.659666Z","shell.execute_reply.started":"2023-08-09T05:08:50.477728Z","shell.execute_reply":"2023-08-09T05:08:54.658285Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"for inputs, targets in binary_1gram_train_ds:\n    print(\"inputs.shape:\", inputs.shape)\n    print(\"inputs.dtype:\", inputs.dtype)\n    print(\"targets.shape:\", targets.shape)\n    print(\"targets.dtype:\", targets.dtype)\n    print(\"inputs[0]:\", inputs[0])\n    print(\"targets[0]:\", targets[0])\n    break","metadata":{"execution":{"iopub.status.busy":"2023-08-09T05:08:59.820209Z","iopub.execute_input":"2023-08-09T05:08:59.820637Z","iopub.status.idle":"2023-08-09T05:08:59.949744Z","shell.execute_reply.started":"2023-08-09T05:08:59.820601Z","shell.execute_reply":"2023-08-09T05:08:59.948447Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"inputs.shape: (32, 20000)\ninputs.dtype: <dtype: 'float32'>\ntargets.shape: (32,)\ntargets.dtype: <dtype: 'int32'>\ninputs[0]: tf.Tensor([1. 1. 1. ... 0. 0. 0.], shape=(20000,), dtype=float32)\ntargets[0]: tf.Tensor(1, shape=(), dtype=int32)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Next, let’s write a reusable model-building function that we’ll use in all of our experiments in this section.\n\n### Our model-building utility","metadata":{}},{"cell_type":"code","source":"from tensorflow import keras\nfrom tensorflow.keras import layers\n\ndef get_model(max_tokens=20000, hidden_dim=16):\n    inputs = keras.Input(shape=(max_tokens,))\n    x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n    x = layers.Dropout(0.5)(x)\n    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n    model = keras.Model(inputs, outputs)\n    model.compile(optimizer=\"rmsprop\", \n                  loss=\"binary_crossentropy\",\n                  metrics=[\"accuracy\"])\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-08-09T05:09:03.767077Z","iopub.execute_input":"2023-08-09T05:09:03.767515Z","iopub.status.idle":"2023-08-09T05:09:03.776726Z","shell.execute_reply.started":"2023-08-09T05:09:03.767478Z","shell.execute_reply":"2023-08-09T05:09:03.775165Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"### Training and testing the binary unigram model\n\nIn `model.fit()`, we call `cache()` on the\ndatasets to cache them in\nmemory: this way, we will\nonly do the preprocessing\nonce, during the first\nepoch, and we’ll reuse the\npreprocessed texts for the\nfollowing epochs. This can\nonly be done if the data\nis small enough to fit in\nmemory.","metadata":{}},{"cell_type":"code","source":"model = get_model()\nmodel.summary()\ncallbacks = [\n    keras.callbacks.ModelCheckpoint(\"binary_1gram.keras\",\n                                   save_best_only=True)\n]\nmodel.fit(binary_1gram_train_ds.cache(), \n         validation_data=binary_1gram_val_ds.cache(),\n         epochs=10,\n         callbacks=callbacks)\nmodel = keras.models.load_model(\"binary_1gram.keras\")\nprint(f\"Test acc: {model.evaluate(binary_1gram_test_ds)[1]:.3f}\")","metadata":{"execution":{"iopub.status.busy":"2023-08-09T05:09:07.450869Z","iopub.execute_input":"2023-08-09T05:09:07.451337Z","iopub.status.idle":"2023-08-09T05:09:58.467009Z","shell.execute_reply.started":"2023-08-09T05:09:07.451299Z","shell.execute_reply":"2023-08-09T05:09:58.465628Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Model: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 20000)]           0         \n                                                                 \n dense (Dense)               (None, 16)                320016    \n                                                                 \n dropout (Dropout)           (None, 16)                0         \n                                                                 \n dense_1 (Dense)             (None, 1)                 17        \n                                                                 \n=================================================================\nTotal params: 320,033\nTrainable params: 320,033\nNon-trainable params: 0\n_________________________________________________________________\nEpoch 1/10\n625/625 [==============================] - 7s 9ms/step - loss: 0.4288 - accuracy: 0.8174 - val_loss: 0.2942 - val_accuracy: 0.8844\nEpoch 2/10\n625/625 [==============================] - 4s 6ms/step - loss: 0.2881 - accuracy: 0.8927 - val_loss: 0.2821 - val_accuracy: 0.8862\nEpoch 3/10\n625/625 [==============================] - 4s 7ms/step - loss: 0.2505 - accuracy: 0.9128 - val_loss: 0.2950 - val_accuracy: 0.8870\nEpoch 4/10\n625/625 [==============================] - 4s 6ms/step - loss: 0.2339 - accuracy: 0.9189 - val_loss: 0.3072 - val_accuracy: 0.8834\nEpoch 5/10\n625/625 [==============================] - 4s 6ms/step - loss: 0.2237 - accuracy: 0.9255 - val_loss: 0.3229 - val_accuracy: 0.8810\nEpoch 6/10\n625/625 [==============================] - 4s 6ms/step - loss: 0.2146 - accuracy: 0.9276 - val_loss: 0.3383 - val_accuracy: 0.8798\nEpoch 7/10\n625/625 [==============================] - 4s 6ms/step - loss: 0.2061 - accuracy: 0.9334 - val_loss: 0.3493 - val_accuracy: 0.8768\nEpoch 8/10\n625/625 [==============================] - 4s 6ms/step - loss: 0.2047 - accuracy: 0.9345 - val_loss: 0.3551 - val_accuracy: 0.8756\nEpoch 9/10\n625/625 [==============================] - 4s 6ms/step - loss: 0.1961 - accuracy: 0.9374 - val_loss: 0.3705 - val_accuracy: 0.8770\nEpoch 10/10\n625/625 [==============================] - 4s 7ms/step - loss: 0.1918 - accuracy: 0.9373 - val_loss: 0.3807 - val_accuracy: 0.8774\n782/782 [==============================] - 3s 3ms/step - loss: 0.2844 - accuracy: 0.8874\nTest acc: 0.887\n","output_type":"stream"}]},{"cell_type":"markdown","source":"This gets us to a test accuracy of 88.7%: not bad! Note that in this case, since the dataset is a balanced two-class classification dataset (there are as many positive samples as\nnegative samples), the “naive baseline” we could reach without training an actual model\nwould only be 50%. Meanwhile, the best score that can be achieved on this dataset\nwithout leveraging external data is around 95% test accuracy. ","metadata":{}},{"cell_type":"markdown","source":"**BIGRAMS WITH BINARY ENCODING**\n\nOf course, discarding word order is very reductive, because even atomic concepts can\nbe expressed via multiple words: the term “United States” conveys a concept that is\nquite distinct from the meaning of the words “states” and “united” taken separately.\nFor this reason, you will usually end up re-injecting local order information into your\nbag-of-words representation by looking at N-grams rather than single words (most\ncommonly, bigrams).\n With bigrams, our sentence becomes\n```\n{\"the\", \"the cat\", \"cat\", \"cat sat\", \"sat\",\n \"sat on\", \"on\", \"on the\", \"the mat\", \"mat\"}\n```\nThe `TextVectorization` layer can be configured to return arbitrary N-grams: bigrams,\ntrigrams, etc. Just pass an `ngrams=N` argument as in the following listing.\n\n### Configuring the `TextVectorization` layer to return bigrams","metadata":{}},{"cell_type":"code","source":"text_vectorization = TextVectorization(\n    ngrams=2,\n    max_tokens=20000,\n    output_mode=\"multi_hot\",\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-09T05:18:07.621682Z","iopub.execute_input":"2023-08-09T05:18:07.622256Z","iopub.status.idle":"2023-08-09T05:18:07.637552Z","shell.execute_reply.started":"2023-08-09T05:18:07.622211Z","shell.execute_reply":"2023-08-09T05:18:07.636428Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"### Training and testing the binary bigram model","metadata":{}},{"cell_type":"code","source":"text_vectorization.adapt(text_only_train_ds)\nbinary_2gram_train_ds = train_ds.map(\n    lambda x, y: (text_vectorization(x), y),\n    num_parallel_calls=4)\nbinary_2gram_val_ds = val_ds.map(\n    lambda x, y: (text_vectorization(x), y),\n    num_parallel_calls=4)\nbinary_2gram_test_ds = test_ds.map(\n    lambda x, y: (text_vectorization(x), y),\n    num_parallel_calls=4)\nmodel = get_model()\nmodel.summary()\ncallbacks = [\n    keras.callbacks.ModelCheckpoint(\"binary_2gram.keras\",\n                                    save_best_only=True)\n]\nmodel.fit(binary_2gram_train_ds.cache(),\n        validation_data=binary_2gram_val_ds.cache(),\n        epochs=10,\n        callbacks=callbacks)\nmodel = keras.models.load_model(\"binary_2gram.keras\")\nprint(f\"Test acc: {model.evaluate(binary_2gram_test_ds)[1]:.3f}\")","metadata":{"execution":{"iopub.status.busy":"2023-08-09T05:19:39.172919Z","iopub.execute_input":"2023-08-09T05:19:39.173433Z","iopub.status.idle":"2023-08-09T05:20:50.685823Z","shell.execute_reply.started":"2023-08-09T05:19:39.173387Z","shell.execute_reply":"2023-08-09T05:20:50.684030Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Model: \"model_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_2 (InputLayer)        [(None, 20000)]           0         \n                                                                 \n dense_2 (Dense)             (None, 16)                320016    \n                                                                 \n dropout_1 (Dropout)         (None, 16)                0         \n                                                                 \n dense_3 (Dense)             (None, 1)                 17        \n                                                                 \n=================================================================\nTotal params: 320,033\nTrainable params: 320,033\nNon-trainable params: 0\n_________________________________________________________________\nEpoch 1/10\n625/625 [==============================] - 7s 9ms/step - loss: 0.3930 - accuracy: 0.8378 - val_loss: 0.2807 - val_accuracy: 0.8896\nEpoch 2/10\n625/625 [==============================] - 4s 7ms/step - loss: 0.2525 - accuracy: 0.9122 - val_loss: 0.2765 - val_accuracy: 0.8912\nEpoch 3/10\n625/625 [==============================] - 4s 7ms/step - loss: 0.2135 - accuracy: 0.9315 - val_loss: 0.2917 - val_accuracy: 0.8918\nEpoch 4/10\n625/625 [==============================] - 4s 6ms/step - loss: 0.1970 - accuracy: 0.9365 - val_loss: 0.3088 - val_accuracy: 0.8920\nEpoch 5/10\n625/625 [==============================] - 4s 6ms/step - loss: 0.1863 - accuracy: 0.9428 - val_loss: 0.3239 - val_accuracy: 0.8940\nEpoch 6/10\n625/625 [==============================] - 4s 6ms/step - loss: 0.1618 - accuracy: 0.9517 - val_loss: 0.3576 - val_accuracy: 0.8890\nEpoch 7/10\n625/625 [==============================] - 4s 6ms/step - loss: 0.1672 - accuracy: 0.9530 - val_loss: 0.3674 - val_accuracy: 0.8840\nEpoch 8/10\n625/625 [==============================] - 4s 6ms/step - loss: 0.1604 - accuracy: 0.9553 - val_loss: 0.3808 - val_accuracy: 0.8870\nEpoch 9/10\n625/625 [==============================] - 4s 6ms/step - loss: 0.1543 - accuracy: 0.9594 - val_loss: 0.3952 - val_accuracy: 0.8824\nEpoch 10/10\n625/625 [==============================] - 4s 6ms/step - loss: 0.1489 - accuracy: 0.9589 - val_loss: 0.3890 - val_accuracy: 0.8786\n782/782 [==============================] - 3s 4ms/step - loss: 0.2694 - accuracy: 0.8999\nTest acc: 0.900\n","output_type":"stream"}]},{"cell_type":"markdown","source":"We’re now getting 90% test accuracy, a marked improvement! Turns out local order\nis pretty important. ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}