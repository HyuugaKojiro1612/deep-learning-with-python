{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 11.3 Two approaches for representing groups of words: Sets and sequences","metadata":{}},{"cell_type":"markdown","source":"How a machine learning model should represent individual words is a relatively uncontroversial question: they’re categorical features (values from a predefined set), and we\nknow how to handle those. They should be encoded as dimensions in a feature space,\nor as category vectors (word vectors in this case). A much more problematic question,\nhowever, is how to encode *the way words are woven into sentences*: word order.\n\nThe problem of order in natural language is an interesting one: unlike the steps of\na timeseries, words in a sentence don’t have a natural, canonical order. Different languages order similar words in very different ways. For instance, the sentence structure\nof English is quite different from that of Japanese. Even within a given language, you\ncan typically say the same thing in different ways by reshuffling the words a bit. Even\nfurther, if you fully randomize the words in a short sentence, you can still largely figure out what it was saying—though in many cases significant ambiguity seems to arise.\nOrder is clearly important, but its relationship to meaning isn’t straightforward.\n\n How to represent word order is the pivotal question from which different kinds of\nNLP architectures spring. The simplest thing you could do is just discard order and\ntreat text as an unordered set of words—this gives you *bag-of-words models*. You could\nalso decide that words should be processed strictly in the order in which they appear,\none at a time, like steps in a timeseries—you could then leverage the recurrent models\nfrom the last chapter. Finally, a hybrid approach is also possible: the Transformer architecture is technically order-agnostic, yet it injects word-position information into\nthe representations it processes, which enables it to simultaneously look at different\nparts of a sentence (unlike RNNs) while still being order-aware. Because they take into\naccount word order, both RNNs and Transformers are called *sequence models*.\n\nHistorically, most early applications of machine learning to NLP just involved\nbag-of-words models. Interest in sequence models only started rising in 2015, with the\nrebirth of recurrent neural networks. Today, both approaches remain relevant. Let’s\nsee how they work, and when to leverage which.\n\nWe’ll demonstrate each approach on a well-known text classification benchmark:\nthe IMDB movie review sentiment-classification dataset. In chapters 4 and 5, you\nworked with a prevectorized version of the IMDB dataset; now, let’s process the raw\nIMDB text data, just like you would do when approaching a new text-classification\nproblem in the real world.","metadata":{}},{"cell_type":"markdown","source":"## 11.3.1 Preparing the IMDB movie reviews data\n\nLet’s start by downloading the dataset from the Stanford page of Andrew Maas and\nuncompressing it:","metadata":{}},{"cell_type":"code","source":"!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n!tar -xf aclImdb_v1.tar.gz\n\n!rm -r aclImdb/train/unsup","metadata":{"execution":{"iopub.status.busy":"2023-08-10T06:15:45.361190Z","iopub.execute_input":"2023-08-10T06:15:45.361615Z","iopub.status.idle":"2023-08-10T06:16:00.170779Z","shell.execute_reply.started":"2023-08-10T06:15:45.361580Z","shell.execute_reply":"2023-08-10T06:16:00.169312Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"curl: /opt/conda/lib/libcurl.so.4: no version information available (required by curl)\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100 80.2M  100 80.2M    0     0  17.8M      0  0:00:04  0:00:04 --:--:-- 17.8M\n","output_type":"stream"}]},{"cell_type":"markdown","source":"You’re left with a directory named aclImdb. The train/pos/ directory contains a set of 12,500 text files, each of which\ncontains the text body of a positive-sentiment movie review to be used as training data.\nThe negative-sentiment reviews live in the “neg” directories. In total, there are 25,000\ntext files for training and another 25,000 for testing.\n\nTake a look at the content of a few of these text files. Whether you’re working with\ntext data or image data, remember to always inspect what your data looks like before\nyou dive into modeling it. It will ground your intuition about what your model is actually doing:","metadata":{}},{"cell_type":"code","source":"!cat aclImdb/train/pos/4077_10.txt","metadata":{"execution":{"iopub.status.busy":"2023-08-10T06:16:27.236355Z","iopub.execute_input":"2023-08-10T06:16:27.236794Z","iopub.status.idle":"2023-08-10T06:16:28.359375Z","shell.execute_reply.started":"2023-08-10T06:16:27.236755Z","shell.execute_reply":"2023-08-10T06:16:28.357778Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"I first saw this back in the early 90s on UK TV, i did like it then but i missed the chance to tape it, many years passed but the film always stuck with me and i lost hope of seeing it TV again, the main thing that stuck with me was the end, the hole castle part really touched me, its easy to watch, has a great story, great music, the list goes on and on, its OK me saying how good it is but everyone will take there own best bits away with them once they have seen it, yes the animation is top notch and beautiful to watch, it does show its age in a very few parts but that has now become part of it beauty, i am so glad it has came out on DVD as it is one of my top 10 films of all time. Buy it or rent it just see it, best viewing is at night alone with drink and food in reach so you don't have to stop the film.<br /><br />Enjoy","output_type":"stream"}]},{"cell_type":"markdown","source":"Next, let’s prepare a validation set by setting apart 20% of the training text files in a\nnew directory, aclImdb/val:","metadata":{}},{"cell_type":"code","source":"import os, pathlib, shutil, random\n\nbase_dir = pathlib.Path(\"aclImdb\")\nval_dir = base_dir / \"val\"\ntrain_dir = base_dir / \"train\"\nfor category in (\"neg\", \"pos\"):\n    os.makedirs(val_dir / category)\n    files = os.listdir(train_dir / category)\n    # Shuffle the list of training files using a seed, to ensure we get the same validation set every time we run the code.\n    random.Random(1337).shuffle(files)\n    # Take 20% of the training files to use for validation.\n    num_val_samples = int(0.2 * len(files))\n    val_files = files[-num_val_samples:]\n    # Move the files to aclImdb/val/neg and aclImdb/val/pos.\n    for fname in val_files:\n        shutil.move(train_dir / category / fname, val_dir / category / fname)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T06:16:36.008884Z","iopub.execute_input":"2023-08-10T06:16:36.009994Z","iopub.status.idle":"2023-08-10T06:16:36.371357Z","shell.execute_reply.started":"2023-08-10T06:16:36.009929Z","shell.execute_reply":"2023-08-10T06:16:36.370196Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"Remember how, in chapter 8, we used the `image_dataset_from_directory` utility to\ncreate a batched `Dataset` of images and their labels for a directory structure? You can\ndo the exact same thing for text files using the `text_dataset_from_directory` utility.\nLet’s create three Dataset objects for training, validation, and testing:","metadata":{}},{"cell_type":"code","source":"from tensorflow import keras","metadata":{"execution":{"iopub.status.busy":"2023-08-10T06:16:55.676722Z","iopub.execute_input":"2023-08-10T06:16:55.677543Z","iopub.status.idle":"2023-08-10T06:16:55.683244Z","shell.execute_reply.started":"2023-08-10T06:16:55.677505Z","shell.execute_reply":"2023-08-10T06:16:55.681747Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"Running `train_ds = ...` line should\noutput “Found 20000 files\nbelonging to 2 classes”;\nif you see “Found 70000\nfiles belonging to 3\nclasses,” it means you\nforgot to delete the\naclImdb/train/unsup\ndirectory.","metadata":{}},{"cell_type":"code","source":"batch_size = 32\n\ntrain_ds = keras.utils.text_dataset_from_directory(\n    \"aclImdb/train\", batch_size=batch_size\n)\nval_ds = keras.utils.text_dataset_from_directory(\n    \"aclImdb/val\", batch_size=batch_size\n)\ntest_ds = keras.utils.text_dataset_from_directory(\n    \"aclImdb/test\", batch_size=batch_size\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T06:16:57.981696Z","iopub.execute_input":"2023-08-10T06:16:57.982139Z","iopub.status.idle":"2023-08-10T06:17:05.696611Z","shell.execute_reply.started":"2023-08-10T06:16:57.982104Z","shell.execute_reply":"2023-08-10T06:17:05.695518Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Found 20000 files belonging to 2 classes.\nFound 5000 files belonging to 2 classes.\nFound 25000 files belonging to 2 classes.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"These datasets yield inputs that are TensorFlow `tf.string` tensors and targets that are `int32` tensors encoding the value “0” or “1.”\n\n### Displaying the shapes and dtypes of the first batch","metadata":{}},{"cell_type":"code","source":"for inputs, targets in train_ds:\n    print(\"inputs.shape:\", inputs.shape)\n    print(\"inputs.dtype:\", inputs.dtype)\n    print(\"targets.shape:\", targets.shape)\n    print(\"targets.dtype:\", targets.dtype)\n    print(\"inputs[0]:\", inputs[0])\n    print(\"targets[0]:\", targets[0])\n    break","metadata":{"execution":{"iopub.status.busy":"2023-08-10T06:17:08.656762Z","iopub.execute_input":"2023-08-10T06:17:08.657184Z","iopub.status.idle":"2023-08-10T06:17:08.770157Z","shell.execute_reply.started":"2023-08-10T06:17:08.657148Z","shell.execute_reply":"2023-08-10T06:17:08.769053Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"inputs.shape: (32,)\ninputs.dtype: <dtype: 'string'>\ntargets.shape: (32,)\ntargets.dtype: <dtype: 'int32'>\ninputs[0]: tf.Tensor(b\"Matt Cordell is back from the dead for a third go-round, although I'm not sure anyone cared at this point except for rabid MANICA COP fans. Cordell, who died in the last flick, is resurrected through voodoo, and is now hot on the trail of several miscreants involved in the shooting of a fellow officer Cordell is very fond of. I missed part of this early '90s low-budget quickie, but it was pleasing to see Cordell wracking up the body count in various, gruesome ways. Problem is, the overall film is pretty static, and Cordell simply ain't Jason or Freddy. The interest wanes pretty fast, even with that grand B-movie master Robert Forster as a doctor who ends up with his brains scrambled. Stick with the first film in the series, which is funny and scary and exciting, all at the same time.\", shape=(), dtype=string)\ntargets[0]: tf.Tensor(0, shape=(), dtype=int32)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 11.3.2 Processing words as a set: The bag-of-words approach\n\nThe simplest way to encode a piece of text for processing by a machine learning\nmodel is to discard order and treat it as a set (a “bag”) of tokens. You could either look\nat individual words (unigrams), or try to recover some local order information by\nlooking at groups of consecutive token (N-grams).\n\n**SINGLE WORDS (UNIGRAMS) WITH BINARY ENCODING**\n\nIf you use a bag of single words, the sentence “the cat sat on the mat” becomes\n```\n{\"cat\", \"mat\", \"on\", \"sat\", \"the\"}\n```\nThe main advantage of this encoding is that you can represent an entire text as a single vector, where each entry is a presence indicator for a given word. For instance,\nusing binary encoding (multi-hot), you’d encode a text as a vector with as many\ndimensions as there are words in your vocabulary—with 0s almost everywhere and\nsome 1s for dimensions that encode words present in the text. This is what we did\nwhen we worked with text data in chapters 4 and 5. Let’s try this on our task.\n\nFirst, let’s process our raw text datasets with a `TextVectorization` layer so that\nthey yield multi-hot encoded binary word vectors. Our layer will only look at single\nwords (that is to say, unigrams).\n\n### Preprocessing our datasets with a `TextVectorization` layer\n\nLimit the vocabulary to the 20,000 most frequent words.\nOtherwise we’d be indexing every word in the training data—\npotentially tens of thousands of terms that only occur once or\ntwice and thus aren’t informative. In general, 20,000 is the\nright vocabulary size for text classification.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.layers import TextVectorization\n\ntext_vectorization = TextVectorization(\n    max_tokens=20000,\n    # Encode the output tokens as multi-hot binary vectors.\n    output_mode=\"multi_hot\",\n)\n\n# Prepare a dataset that only yields raw text inputs (no labels).\ntext_only_train_ds = train_ds.map(lambda x, y: x)\n# Use that dataset to index the dataset vocabulary via the adapt() method.\ntext_vectorization.adapt(text_only_train_ds)\n\n# Prepare processed versions of our training, validation, and test dataset.\n# Make sure to specify num_parallel_calls to leverage multiple CPU cores.\nbinary_1gram_train_ds = train_ds.map(\n    lambda x, y: (text_vectorization(x), y),\n    num_parallel_calls=4\n)\nbinary_1gram_val_ds = val_ds.map(\n    lambda x, y: (text_vectorization(x), y),\n    num_parallel_calls=4\n)\nbinary_1gram_test_ds = test_ds.map(\n    lambda x, y: (text_vectorization(x), y),\n    num_parallel_calls=4\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T06:17:12.198209Z","iopub.execute_input":"2023-08-10T06:17:12.198663Z","iopub.status.idle":"2023-08-10T06:17:17.956549Z","shell.execute_reply.started":"2023-08-10T06:17:12.198628Z","shell.execute_reply":"2023-08-10T06:17:17.955518Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"for inputs, targets in binary_1gram_train_ds:\n    print(\"inputs.shape:\", inputs.shape)\n    print(\"inputs.dtype:\", inputs.dtype)\n    print(\"targets.shape:\", targets.shape)\n    print(\"targets.dtype:\", targets.dtype)\n    print(\"inputs[0]:\", inputs[0])\n    print(\"targets[0]:\", targets[0])\n    break","metadata":{"execution":{"iopub.status.busy":"2023-08-10T06:17:26.054732Z","iopub.execute_input":"2023-08-10T06:17:26.055170Z","iopub.status.idle":"2023-08-10T06:17:26.262918Z","shell.execute_reply.started":"2023-08-10T06:17:26.055132Z","shell.execute_reply":"2023-08-10T06:17:26.261866Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"inputs.shape: (32, 20000)\ninputs.dtype: <dtype: 'float32'>\ntargets.shape: (32,)\ntargets.dtype: <dtype: 'int32'>\ninputs[0]: tf.Tensor([1. 1. 1. ... 0. 0. 0.], shape=(20000,), dtype=float32)\ntargets[0]: tf.Tensor(0, shape=(), dtype=int32)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Next, let’s write a reusable model-building function that we’ll use in all of our experiments in this section.\n\n### Our model-building utility","metadata":{}},{"cell_type":"code","source":"from tensorflow import keras\nfrom tensorflow.keras import layers\n\ndef get_model(max_tokens=20000, hidden_dim=16):\n    inputs = keras.Input(shape=(max_tokens,))\n    x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n    x = layers.Dropout(0.5)(x)\n    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n    model = keras.Model(inputs, outputs)\n    model.compile(optimizer=\"rmsprop\", \n                  loss=\"binary_crossentropy\",\n                  metrics=[\"accuracy\"])\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-08-10T06:17:29.429189Z","iopub.execute_input":"2023-08-10T06:17:29.429615Z","iopub.status.idle":"2023-08-10T06:17:29.438395Z","shell.execute_reply.started":"2023-08-10T06:17:29.429577Z","shell.execute_reply":"2023-08-10T06:17:29.436942Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"### Training and testing the binary unigram model\n\nIn `model.fit()`, we call `cache()` on the\ndatasets to cache them in\nmemory: this way, we will\nonly do the preprocessing\nonce, during the first\nepoch, and we’ll reuse the\npreprocessed texts for the\nfollowing epochs. This can\nonly be done if the data\nis small enough to fit in\nmemory.","metadata":{}},{"cell_type":"code","source":"model = get_model()\nmodel.summary()\ncallbacks = [\n    keras.callbacks.ModelCheckpoint(\"binary_1gram.keras\",\n                                   save_best_only=True)\n]\nmodel.fit(binary_1gram_train_ds.cache(), \n         validation_data=binary_1gram_val_ds.cache(),\n         epochs=10,\n         callbacks=callbacks)\nmodel = keras.models.load_model(\"binary_1gram.keras\")\nprint(f\"Test acc: {model.evaluate(binary_1gram_test_ds)[1]:.3f}\")","metadata":{"execution":{"iopub.status.busy":"2023-08-10T06:17:32.499517Z","iopub.execute_input":"2023-08-10T06:17:32.499943Z","iopub.status.idle":"2023-08-10T06:18:22.486169Z","shell.execute_reply.started":"2023-08-10T06:17:32.499879Z","shell.execute_reply":"2023-08-10T06:18:22.484966Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Model: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 20000)]           0         \n                                                                 \n dense (Dense)               (None, 16)                320016    \n                                                                 \n dropout (Dropout)           (None, 16)                0         \n                                                                 \n dense_1 (Dense)             (None, 1)                 17        \n                                                                 \n=================================================================\nTotal params: 320,033\nTrainable params: 320,033\nNon-trainable params: 0\n_________________________________________________________________\nEpoch 1/10\n625/625 [==============================] - 11s 11ms/step - loss: 0.4196 - accuracy: 0.8231 - val_loss: 0.2784 - val_accuracy: 0.8932\nEpoch 2/10\n625/625 [==============================] - 3s 5ms/step - loss: 0.2840 - accuracy: 0.8934 - val_loss: 0.2663 - val_accuracy: 0.8944\nEpoch 3/10\n625/625 [==============================] - 3s 5ms/step - loss: 0.2464 - accuracy: 0.9087 - val_loss: 0.2708 - val_accuracy: 0.9012\nEpoch 4/10\n625/625 [==============================] - 4s 6ms/step - loss: 0.2299 - accuracy: 0.9220 - val_loss: 0.2898 - val_accuracy: 0.8948\nEpoch 5/10\n625/625 [==============================] - 3s 5ms/step - loss: 0.2209 - accuracy: 0.9263 - val_loss: 0.2996 - val_accuracy: 0.8966\nEpoch 6/10\n625/625 [==============================] - 3s 5ms/step - loss: 0.2138 - accuracy: 0.9303 - val_loss: 0.3083 - val_accuracy: 0.8954\nEpoch 7/10\n625/625 [==============================] - 3s 5ms/step - loss: 0.2081 - accuracy: 0.9335 - val_loss: 0.3156 - val_accuracy: 0.8968\nEpoch 8/10\n625/625 [==============================] - 3s 5ms/step - loss: 0.2048 - accuracy: 0.9352 - val_loss: 0.3337 - val_accuracy: 0.8968\nEpoch 9/10\n625/625 [==============================] - 3s 5ms/step - loss: 0.2079 - accuracy: 0.9362 - val_loss: 0.3347 - val_accuracy: 0.8964\nEpoch 10/10\n625/625 [==============================] - 3s 5ms/step - loss: 0.1982 - accuracy: 0.9370 - val_loss: 0.3438 - val_accuracy: 0.8950\n782/782 [==============================] - 5s 6ms/step - loss: 0.2869 - accuracy: 0.8866\nTest acc: 0.887\n","output_type":"stream"}]},{"cell_type":"markdown","source":"This gets us to a test accuracy of 88.7%: not bad! Note that in this case, since the dataset is a balanced two-class classification dataset (there are as many positive samples as\nnegative samples), the “naive baseline” we could reach without training an actual model\nwould only be 50%. Meanwhile, the best score that can be achieved on this dataset\nwithout leveraging external data is around 95% test accuracy. ","metadata":{}},{"cell_type":"markdown","source":"**BIGRAMS WITH BINARY ENCODING**\n\nOf course, discarding word order is very reductive, because even atomic concepts can\nbe expressed via multiple words: the term “United States” conveys a concept that is\nquite distinct from the meaning of the words “states” and “united” taken separately.\nFor this reason, you will usually end up re-injecting local order information into your\nbag-of-words representation by looking at N-grams rather than single words (most\ncommonly, bigrams).\n With bigrams, our sentence becomes\n```\n{\"the\", \"the cat\", \"cat\", \"cat sat\", \"sat\",\n \"sat on\", \"on\", \"on the\", \"the mat\", \"mat\"}\n```\nThe `TextVectorization` layer can be configured to return arbitrary N-grams: bigrams,\ntrigrams, etc. Just pass an `ngrams=N` argument as in the following listing.\n\n### Configuring the `TextVectorization` layer to return bigrams","metadata":{}},{"cell_type":"code","source":"text_vectorization = TextVectorization(\n    ngrams=2,\n    max_tokens=20000,\n    output_mode=\"multi_hot\",\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T06:21:42.191023Z","iopub.execute_input":"2023-08-10T06:21:42.191418Z","iopub.status.idle":"2023-08-10T06:21:42.203556Z","shell.execute_reply.started":"2023-08-10T06:21:42.191385Z","shell.execute_reply":"2023-08-10T06:21:42.202378Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"### Training and testing the binary bigram model","metadata":{}},{"cell_type":"code","source":"text_vectorization.adapt(text_only_train_ds)\nbinary_2gram_train_ds = train_ds.map(\n    lambda x, y: (text_vectorization(x), y),\n    num_parallel_calls=4)\nbinary_2gram_val_ds = val_ds.map(\n    lambda x, y: (text_vectorization(x), y),\n    num_parallel_calls=4)\nbinary_2gram_test_ds = test_ds.map(\n    lambda x, y: (text_vectorization(x), y),\n    num_parallel_calls=4)\nmodel = get_model()\nmodel.summary()\ncallbacks = [\n    keras.callbacks.ModelCheckpoint(\"binary_2gram.keras\",\n                                    save_best_only=True)\n]\nmodel.fit(binary_2gram_train_ds.cache(),\n        validation_data=binary_2gram_val_ds.cache(),\n        epochs=10,\n        callbacks=callbacks)\nmodel = keras.models.load_model(\"binary_2gram.keras\")\nprint(f\"Test acc: {model.evaluate(binary_2gram_test_ds)[1]:.3f}\")","metadata":{"execution":{"iopub.status.busy":"2023-08-10T06:21:44.431013Z","iopub.execute_input":"2023-08-10T06:21:44.432032Z","iopub.status.idle":"2023-08-10T06:22:46.610239Z","shell.execute_reply.started":"2023-08-10T06:21:44.431994Z","shell.execute_reply":"2023-08-10T06:22:46.608866Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Model: \"model_3\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_4 (InputLayer)        [(None, 20000)]           0         \n                                                                 \n dense_6 (Dense)             (None, 16)                320016    \n                                                                 \n dropout_3 (Dropout)         (None, 16)                0         \n                                                                 \n dense_7 (Dense)             (None, 1)                 17        \n                                                                 \n=================================================================\nTotal params: 320,033\nTrainable params: 320,033\nNon-trainable params: 0\n_________________________________________________________________\nEpoch 1/10\n625/625 [==============================] - 8s 11ms/step - loss: 0.3947 - accuracy: 0.8332 - val_loss: 0.2551 - val_accuracy: 0.9010\nEpoch 2/10\n625/625 [==============================] - 4s 6ms/step - loss: 0.2475 - accuracy: 0.9119 - val_loss: 0.2532 - val_accuracy: 0.9050\nEpoch 3/10\n625/625 [==============================] - 3s 5ms/step - loss: 0.2094 - accuracy: 0.9323 - val_loss: 0.2709 - val_accuracy: 0.9012\nEpoch 4/10\n625/625 [==============================] - 3s 5ms/step - loss: 0.1872 - accuracy: 0.9428 - val_loss: 0.2859 - val_accuracy: 0.9004\nEpoch 5/10\n625/625 [==============================] - 3s 5ms/step - loss: 0.1745 - accuracy: 0.9474 - val_loss: 0.3017 - val_accuracy: 0.9018\nEpoch 6/10\n625/625 [==============================] - 3s 5ms/step - loss: 0.1668 - accuracy: 0.9516 - val_loss: 0.3079 - val_accuracy: 0.8994\nEpoch 7/10\n625/625 [==============================] - 3s 5ms/step - loss: 0.1536 - accuracy: 0.9563 - val_loss: 0.3251 - val_accuracy: 0.9040\nEpoch 8/10\n625/625 [==============================] - 3s 5ms/step - loss: 0.1596 - accuracy: 0.9571 - val_loss: 0.3341 - val_accuracy: 0.8986\nEpoch 9/10\n625/625 [==============================] - 3s 5ms/step - loss: 0.1502 - accuracy: 0.9602 - val_loss: 0.3444 - val_accuracy: 0.9000\nEpoch 10/10\n625/625 [==============================] - 3s 5ms/step - loss: 0.1450 - accuracy: 0.9616 - val_loss: 0.3534 - val_accuracy: 0.8964\n782/782 [==============================] - 6s 8ms/step - loss: 0.2617 - accuracy: 0.9004\nTest acc: 0.900\n","output_type":"stream"}]},{"cell_type":"markdown","source":"We’re now getting 90% test accuracy, a marked improvement! Turns out local order\nis pretty important. ","metadata":{}},{"cell_type":"markdown","source":"**BIGRAMS WITH TF-IDF ENCODING**\n\nYou can also add a bit more information to this representation by counting how many\ntimes each word or N-gram occurs, that is to say, by taking the histogram of the words\nover the text:\n```\n{\"the\": 2, \"the cat\": 1, \"cat\": 1, \"cat sat\": 1, \"sat\": 1,\n \"sat on\": 1, \"on\": 1, \"on the\": 1, \"the mat: 1\", \"mat\": 1}\n```\nIf you’re doing text classification, knowing how many times a word occurs in a sample\nis critical: any sufficiently long movie review may contain the word “terrible” regardless of sentiment, but a review that contains many instances of the word “terrible” is\nlikely a negative one.\n Here’s how you’d count bigram occurrences with the TextVectorization layer.\n \n###  Configuring the TextVectorization layer to return token counts","metadata":{}},{"cell_type":"code","source":"text_vectorization = TextVectorization(\n    ngrams=2,\n    max_tokens=20000,\n    output_mode=\"count\"\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T06:19:40.733072Z","iopub.execute_input":"2023-08-10T06:19:40.733499Z","iopub.status.idle":"2023-08-10T06:19:40.748957Z","shell.execute_reply.started":"2023-08-10T06:19:40.733455Z","shell.execute_reply":"2023-08-10T06:19:40.747668Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"Now, of course, some words are bound to occur more often than others no matter\nwhat the text is about. The words “the,” “a,” “is,” and “are” will always dominate your\nword count histograms, drowning out other words—despite being pretty much useless\nfeatures in a classification context. How could we address this?\n\n You already guessed it: via normalization. We could just normalize word counts by\nsubtracting the mean and dividing by the variance (computed across the entire training dataset). That would make sense. Except most vectorized sentences consist almost\nentirely of zeros (our previous example features 12 non-zero entries and 19,988 zero\nentries), a property called “sparsity.” That’s a great property to have, as it dramatically\nreduces compute load and reduces the risk of overfitting. If we subtracted the mean\nfrom each feature, we’d wreck sparsity. Thus, whatever normalization scheme we use\nshould be divide-only. What, then, should we use as the denominator? The best practice is to go with something called *TF-IDF normalization*—TF-IDF stands for “term frequency, inverse document frequency.”\n\n TF-IDF is so common that it’s built into the `TextVectorization` layer. All you need\nto do to start using it is to switch the `output_mode` argument to `\"tf_idf\"`.","metadata":{}},{"cell_type":"markdown","source":"**Understanding TF-IDF normalization**\n\nThe more a given term appears in a document, the more important that term is for\nunderstanding what the document is about. At the same time, the frequency at which\nthe term appears across all documents in your dataset matters too: terms that\nappear in almost every document (like “the” or “a”) aren’t particularly informative,\nwhile terms that appear only in a small subset of all texts (like “Herzog”) are very distinctive, and thus important. TF-IDF is a metric that fuses these two ideas. It weights\na given term by taking “term frequency,” how many times the term appears in the\ncurrent document, and dividing it by a measure of “document frequency,” which estimates how often the term comes up across the dataset. You’d compute it as follows:\n``` python\ndef tfidf(term, document, dataset):\n    term_freq = document.count(term)\n    doc_freq = math.log(sum(doc.count(term) for doc in dataset) + 1)\n    return term_freq / doc_freq\n```","metadata":{}},{"cell_type":"markdown","source":"### Configuring `TextVectorization` to return TF-IDF-weighted outputs","metadata":{}},{"cell_type":"code","source":"text_vectorization = TextVectorization(\n    ngrams=2,\n    max_tokens=20000,\n    output_mode=\"tf_idf\",\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T06:19:44.132341Z","iopub.execute_input":"2023-08-10T06:19:44.132767Z","iopub.status.idle":"2023-08-10T06:19:44.154492Z","shell.execute_reply.started":"2023-08-10T06:19:44.132732Z","shell.execute_reply":"2023-08-10T06:19:44.153247Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"### Training and testing the TF-IDF bigram model","metadata":{}},{"cell_type":"code","source":"# The adapt() call will learn the TF-IDF weights in addition to the vocabulary.\ntext_vectorization.adapt(text_only_train_ds)\n\ntfidf_2gram_train_ds = train_ds.map(\n    lambda x, y: (text_vectorization(x), y),\n    num_parallel_calls=4)\ntfidf_2gram_val_ds = val_ds.map(\n    lambda x, y: (text_vectorization(x), y),\n    num_parallel_calls=4)\ntfidf_2gram_test_ds = test_ds.map(\n    lambda x, y: (text_vectorization(x), y),\n    num_parallel_calls=4)\n\nmodel = get_model()\nmodel.summary()\ncallbacks = [\n    keras.callbacks.ModelCheckpoint(\"tfidf_2gram.keras\",\n                                    save_best_only=True)\n]\nmodel.fit(tfidf_2gram_train_ds.cache(),\n        validation_data=tfidf_2gram_val_ds.cache(),\n        epochs=10,\n        callbacks=callbacks)\nmodel = keras.models.load_model(\"tfidf_2gram.keras\")\nprint(f\"Test acc: {model.evaluate(tfidf_2gram_test_ds)[1]:.3f}\")","metadata":{"execution":{"iopub.status.busy":"2023-08-10T06:19:46.647801Z","iopub.execute_input":"2023-08-10T06:19:46.648950Z","iopub.status.idle":"2023-08-10T06:21:19.425329Z","shell.execute_reply.started":"2023-08-10T06:19:46.648878Z","shell.execute_reply":"2023-08-10T06:21:19.424107Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Model: \"model_2\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_3 (InputLayer)        [(None, 20000)]           0         \n                                                                 \n dense_4 (Dense)             (None, 16)                320016    \n                                                                 \n dropout_2 (Dropout)         (None, 16)                0         \n                                                                 \n dense_5 (Dense)             (None, 1)                 17        \n                                                                 \n=================================================================\nTotal params: 320,033\nTrainable params: 320,033\nNon-trainable params: 0\n_________________________________________________________________\nEpoch 1/10\n625/625 [==============================] - 9s 12ms/step - loss: 0.5232 - accuracy: 0.7660 - val_loss: 0.2732 - val_accuracy: 0.8926\nEpoch 2/10\n625/625 [==============================] - 3s 5ms/step - loss: 0.3392 - accuracy: 0.8565 - val_loss: 0.2613 - val_accuracy: 0.8980\nEpoch 3/10\n625/625 [==============================] - 3s 5ms/step - loss: 0.3002 - accuracy: 0.8720 - val_loss: 0.2819 - val_accuracy: 0.8854\nEpoch 4/10\n625/625 [==============================] - 3s 5ms/step - loss: 0.2770 - accuracy: 0.8837 - val_loss: 0.2960 - val_accuracy: 0.8922\nEpoch 5/10\n625/625 [==============================] - 3s 5ms/step - loss: 0.2609 - accuracy: 0.8926 - val_loss: 0.3033 - val_accuracy: 0.8842\nEpoch 6/10\n625/625 [==============================] - 3s 5ms/step - loss: 0.2511 - accuracy: 0.8971 - val_loss: 0.3127 - val_accuracy: 0.8830\nEpoch 7/10\n625/625 [==============================] - 3s 5ms/step - loss: 0.2402 - accuracy: 0.9011 - val_loss: 0.3226 - val_accuracy: 0.8832\nEpoch 8/10\n625/625 [==============================] - 4s 6ms/step - loss: 0.2263 - accuracy: 0.9056 - val_loss: 0.3381 - val_accuracy: 0.8834\nEpoch 9/10\n625/625 [==============================] - 3s 5ms/step - loss: 0.2300 - accuracy: 0.9060 - val_loss: 0.3547 - val_accuracy: 0.8754\nEpoch 10/10\n625/625 [==============================] - 3s 5ms/step - loss: 0.2245 - accuracy: 0.9050 - val_loss: 0.3835 - val_accuracy: 0.8448\n782/782 [==============================] - 5s 7ms/step - loss: 0.2823 - accuracy: 0.8923\nTest acc: 0.892\n","output_type":"stream"}]},{"cell_type":"markdown","source":"This gets us an 88.6% test accuracy on the IMDB classification task: it doesn’t seem to\nbe particularly helpful in this case. However, for many text-classification datasets, it\nwould be typical to see a one-percentage-point increase when using TF-IDF compared\nto plain binary encoding.","metadata":{}},{"cell_type":"markdown","source":"**Exporting a model that processes raw strings**\n\nIn the preceding examples, we did our text standardization, splitting, and indexing as\npart of the tf.data pipeline. But if we want to export a standalone model independent of this pipeline, we should make sure that it incorporates its own text preprocessing (otherwise, you’d have to reimplement in the production environment, which\ncan be challenging or can lead to subtle discrepancies between the training data and\nthe production data). Thankfully, this is easy.\nJust create a new model that reuses your TextVectorization layer and adds to it\nthe model you just trained:","metadata":{}},{"cell_type":"code","source":"# One input sample would be one string.\ninputs = keras.Input(shape=(1,), dtype=\"string\")\n# Apply text preprocessing.\nprocessed_inputs = text_vectorization(inputs)\n# Apply the previously trained model.\noutputs = model(processed_inputs)\n# Instantiate the end-to-end model.\ninference_model = keras.Model(inputs, outputs)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T03:10:48.305144Z","iopub.execute_input":"2023-08-10T03:10:48.305714Z","iopub.status.idle":"2023-08-10T03:10:48.369529Z","shell.execute_reply.started":"2023-08-10T03:10:48.305669Z","shell.execute_reply":"2023-08-10T03:10:48.368360Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# The resulting model can process batches of raw strings:\nimport tensorflow as tf\nraw_text_data = tf.convert_to_tensor([\n [\"That was an excellent movie, I loved it.\"],\n])\npredictions = inference_model(raw_text_data)\nprint(f\"{float(predictions[0] * 100):.2f} percent positive\")","metadata":{"execution":{"iopub.status.busy":"2023-08-10T03:10:51.023547Z","iopub.execute_input":"2023-08-10T03:10:51.024111Z","iopub.status.idle":"2023-08-10T03:10:51.093157Z","shell.execute_reply.started":"2023-08-10T03:10:51.024069Z","shell.execute_reply":"2023-08-10T03:10:51.092127Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"98.93 percent positive\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 11.3.3 Processing words as a sequence: The sequence model approach\n\nThese past few examples clearly show that word order matters: manual engineering of\norder-based features, such as bigrams, yields a nice accuracy boost. Now remember: the\nhistory of deep learning is that of a move away from manual feature engineering, toward\nletting models learn their own features from exposure to data alone. What if, instead of\nmanually crafting order-based features, we exposed the model to raw word sequences\nand let it figure out such features on its own? This is what *sequence models* are about.\n\nTo implement a sequence model, you’d start by representing your input samples as\nsequences of integer indices (one integer standing for one word). Then, you’d map\neach integer to a vector to obtain vector sequences. Finally, you’d feed these\nsequences of vectors into a stack of layers that could cross-correlate features from adjacent vectors, such as a 1D convnet, a RNN, or a Transformer.\n\n For some time around 2016–2017, bidirectional RNNs (in particular, bidirectional\nLSTMs) were considered to be the state of the art for sequence modeling. However, nowadays sequence modeling is almost universally done with Transformers, which we will cover shortly. Oddly, one-dimensional convnets were never\nvery popular in NLP, even though, in my own experience, a residual stack of depthwise-separable 1D convolutions can often achieve comparable performance to a bidirectional LSTM, at a greatly reduced computational cost.","metadata":{}},{"cell_type":"markdown","source":"**A FIRST PRACTICAL EXAMPLE**\n\nLet’s try out a first sequence model in practice. First, let’s prepare datasets that return\ninteger sequences.\n\n### Preparing integer sequence datasets\n\nIn order to keep a manageable\ninput size, we’ll truncate the\ninputs after the first 600 words.\nThis is a reasonable choice, since\nthe average review length is 233\nwords, and only 5% of reviews\nare longer than 600 words.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras import layers\n\nmax_length = 600\nmax_tokens = 20000\ntext_vectorization = layers.TextVectorization(\n    max_tokens=max_tokens,\n    output_mode=\"int\",\n    output_sequence_length=max_length,\n)\ntext_vectorization.adapt(text_only_train_ds)\n\nint_train_ds = train_ds.map(\n    lambda x, y: (text_vectorization(x), y),\n    num_parallel_calls=4)\nint_val_ds = val_ds.map(\n    lambda x, y: (text_vectorization(x), y),\n    num_parallel_calls=4)\nint_test_ds = test_ds.map(\n    lambda x, y: (text_vectorization(x), y),\n    num_parallel_calls=4)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T06:23:43.216115Z","iopub.execute_input":"2023-08-10T06:23:43.216550Z","iopub.status.idle":"2023-08-10T06:23:47.180975Z","shell.execute_reply.started":"2023-08-10T06:23:43.216517Z","shell.execute_reply":"2023-08-10T06:23:47.179196Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"Next, let’s make a model. The simplest way to convert our integer sequences to vector\nsequences is to one-hot encode the integers (each dimension would represent one\npossible term in the vocabulary). On top of these one-hot vectors, we’ll add a simple\nbidirectional LSTM.\n\n### A sequence model built on one-hot encoded vector sequences","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.compat.v1.keras.layers import CuDNNLSTM\n\n# One input is a sequence of integers.\ninputs = keras.Input(shape=(None,), dtype=\"int64\")\n# Encode the integers into binary 20,000-dimensional vectors.\nembedded = tf.one_hot(inputs, depth=max_tokens)\n# Add a bidirectional LSTM.\nx = layers.Bidirectional(CuDNNLSTM(32))(embedded)\nx = layers.Dropout(0.5)(x) \n# Finally, add a classification layer.\noutputs = layers.Dense(1, activation=\"sigmoid\")(x) \nmodel = keras.Model(inputs, outputs)\nmodel.compile(optimizer=\"rmsprop\",\n             loss=\"binary_crossentropy\",\n             metrics=[\"accuracy\"])\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-08-10T06:23:56.238782Z","iopub.execute_input":"2023-08-10T06:23:56.240024Z","iopub.status.idle":"2023-08-10T06:23:57.135152Z","shell.execute_reply.started":"2023-08-10T06:23:56.239972Z","shell.execute_reply":"2023-08-10T06:23:57.134349Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Model: \"model_4\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_5 (InputLayer)        [(None, None)]            0         \n                                                                 \n tf.one_hot (TFOpLambda)     (None, None, 20000)       0         \n                                                                 \n bidirectional (Bidirectiona  (None, 64)               5128704   \n l)                                                              \n                                                                 \n dropout_4 (Dropout)         (None, 64)                0         \n                                                                 \n dense_8 (Dense)             (None, 1)                 65        \n                                                                 \n=================================================================\nTotal params: 5,128,769\nTrainable params: 5,128,769\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Training a first basic sequence model","metadata":{}},{"cell_type":"code","source":"callbacks = [\n    keras.callbacks.ModelCheckpoint(\"one_hot_bidir_lstm.keras\",\n                                    save_best_only=True)\n]\nmodel.fit(int_train_ds, validation_data=int_val_ds, epochs=10,\n         callbacks=callbacks)\nmodel = keras.models.load_model(\"one_hot_bidir_lstm.keras\")\nprint(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")","metadata":{"execution":{"iopub.status.busy":"2023-08-10T06:24:00.494836Z","iopub.execute_input":"2023-08-10T06:24:00.495536Z","iopub.status.idle":"2023-08-10T06:49:57.656203Z","shell.execute_reply.started":"2023-08-10T06:24:00.495502Z","shell.execute_reply":"2023-08-10T06:49:57.654985Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Epoch 1/10\n625/625 [==============================] - 142s 221ms/step - loss: 0.5733 - accuracy: 0.6951 - val_loss: 0.4077 - val_accuracy: 0.8520\nEpoch 2/10\n625/625 [==============================] - 141s 225ms/step - loss: 0.3990 - accuracy: 0.8431 - val_loss: 0.2961 - val_accuracy: 0.8900\nEpoch 3/10\n625/625 [==============================] - 141s 226ms/step - loss: 0.3232 - accuracy: 0.8816 - val_loss: 0.2807 - val_accuracy: 0.8988\nEpoch 4/10\n625/625 [==============================] - 141s 226ms/step - loss: 0.2851 - accuracy: 0.8978 - val_loss: 0.2718 - val_accuracy: 0.8998\nEpoch 5/10\n625/625 [==============================] - 141s 226ms/step - loss: 0.2440 - accuracy: 0.9158 - val_loss: 0.2969 - val_accuracy: 0.8916\nEpoch 6/10\n625/625 [==============================] - 141s 226ms/step - loss: 0.2210 - accuracy: 0.9248 - val_loss: 0.3203 - val_accuracy: 0.8884\nEpoch 7/10\n625/625 [==============================] - 141s 226ms/step - loss: 0.1917 - accuracy: 0.9372 - val_loss: 0.2974 - val_accuracy: 0.8920\nEpoch 8/10\n625/625 [==============================] - 141s 226ms/step - loss: 0.1595 - accuracy: 0.9466 - val_loss: 0.3512 - val_accuracy: 0.8704\nEpoch 9/10\n625/625 [==============================] - 141s 226ms/step - loss: 0.1384 - accuracy: 0.9556 - val_loss: 0.3384 - val_accuracy: 0.8882\nEpoch 10/10\n625/625 [==============================] - 141s 225ms/step - loss: 0.1131 - accuracy: 0.9636 - val_loss: 0.3646 - val_accuracy: 0.8722\n782/782 [==============================] - 88s 112ms/step - loss: 0.3005 - accuracy: 0.8800\nTest acc: 0.880\n","output_type":"stream"}]},{"cell_type":"markdown","source":"A first observation: this model trains very slowly (for roughly a day if not using the CuDNN version), especially compared to the lightweight model of the previous section. This is because our inputs are quite large: each\ninput sample is encoded as a matrix of size `(600, 20000)` (600 words per sample,\n20,000 possible words). That’s 12,000,000 floats for a single movie review. Our bidirectional LSTM has a lot of work to do. Second, the model only gets to 88% test accuracy—it doesn’t perform nearly as well as our (very fast) binary unigram model.\n Clearly, using one-hot encoding to turn words into vectors, which was the simplest\nthing we could do, wasn’t a great idea. There’s a better way: *word embeddings*.","metadata":{}},{"cell_type":"markdown","source":"**UNDERSTANDING WORD EMBEDDINGS**\n\nCrucially, when you encode something via one-hot encoding, you’re making a featureengineering decision. You’re injecting into your model a fundamental assumption\nabout the structure of your feature space. That assumption is that the different tokens\nyou’re encoding are all independent from each other: indeed, one-hot vectors are all orthogonal to one another. And in the case of words, that assumption is clearly wrong. Words\nform a structured space: they share information with each other. The words “movie”\nand “film” are interchangeable in most sentences, so the vector that represents\n“movie” should not be orthogonal to the vector that represents “film”—they should be\nthe same vector, or close enough.\n\nTo get a bit more abstract, the *geometric relationship* between two word vectors\nshould reflect the *semantic relationship* between these words. For instance, in a reasonable word vector space, you would expect synonyms to be embedded into similar word\nvectors, and in general, you would expect the geometric distance (such as the cosine\ndistance or L2 distance) between any two word vectors to relate to the “semantic distance” between the associated words. Words that mean different things should lie far\naway from each other, whereas related words should be closer.\n\n*Word embeddings* are vector representations of words that achieve exactly this: they\nmap human language into a structured geometric space.\n\n Whereas the vectors obtained through one-hot encoding are binary, sparse (mostly\nmade of zeros), and very high-dimensional (the same dimensionality as the number of\nwords in the vocabulary), word embeddings are low-dimensional floating-point vectors\n(that is, dense vectors, as opposed to sparse vectors); see figure 11.2. It’s common to see\nword embeddings that are 256-dimensional, 512-dimensional, or 1,024-dimensional when dealing with very large vocabularies. On the other hand, one-hot encoding words\ngenerally leads to vectors that are 20,000-dimensional or greater (capturing a vocabulary of 20,000 tokens, in this case). So, word embeddings pack more information into\nfar fewer dimensions.\n\n![image.png](attachment:ad8f5044-ee9f-400c-9264-ff57189a6682.png)\n\nBesides being *dense* representations, word embeddings are also *structured* representations, and their structure is learned from data. Similar words get embedded in close\nlocations, and further, specific *directions* in the embedding space are meaningful. In real-world word-embedding spaces, common examples of meaningful geometric\ntransformations are “gender” vectors and “plural” vectors. For instance, by adding a\n“female” vector to the vector “king,” we obtain the vector “queen.” By adding a “plural” vector, we obtain “kings.” Word-embedding spaces typically feature thousands of\nsuch interpretable and potentially useful vectors.\n\nLet’s look at how to use such an embedding space in practice. There are two ways\nto obtain word embeddings:\n- Learn word embeddings jointly with the main task you care about (such as document classification or sentiment prediction). In this setup, you start with random word vectors and then learn word vectors in the same way you learn the\nweights of a neural network.\n- Load into your model word embeddings that were precomputed using a different machine learning task than the one you’re trying to solve. These are called\n*pretrained word embeddings*.\n\nLet’s review each of these approaches. ","metadata":{},"attachments":{"ad8f5044-ee9f-400c-9264-ff57189a6682.png":{"image/png":"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"}}},{"cell_type":"markdown","source":"**LEARNING WORD EMBEDDINGS WITH THE EMBEDDING LAYER**\n\nWhat makes a good word-embedding space depends heavily on\nyour task: the perfect word-embedding space for an English-language movie-review\nsentiment-analysis model may look different from the perfect embedding space for an\nEnglish-language legal-document classification model, because the importance of certain semantic relationships varies from task to task.\n\nIt’s thus reasonable to learn a new embedding space with every new task. Fortunately, backpropagation makes this easy, and Keras makes it even easier. It’s about\nlearning the weights of a layer: the `Embedding` layer.\n\n### Instantiating an `Embedding` layer\nThe `Embedding` layer takes at least two arguments: the number of\npossible tokens and the dimensionality of the embeddings (here, 256).","metadata":{}},{"cell_type":"code","source":"embedding_layer = layers.Embedding(input_dim=max_tokens, output_dim=256)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T06:49:57.658734Z","iopub.execute_input":"2023-08-10T06:49:57.659532Z","iopub.status.idle":"2023-08-10T06:49:57.668626Z","shell.execute_reply.started":"2023-08-10T06:49:57.659476Z","shell.execute_reply":"2023-08-10T06:49:57.667295Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"The `Embedding` layer is best understood as a dictionary that maps integer indices\n(which stand for specific words) to dense vectors. It takes integers as input, looks up\nthese integers in an internal dictionary, and returns the associated vectors. It’s effectively a dictionary lookup.\n\nThe `Embedding` layer takes as input a rank-2 tensor of integers, of shape `(batch_size, sequence_length)`, where each entry is a sequence of integers. The layer then returns\na 3D floating-point tensor of shape `(batch_size, sequence_length, embedding_dimensionality)`.\n\nWhen you instantiate an `Embedding` layer, its weights (its internal dictionary of\ntoken vectors) are initially random, just as with any other layer. During training, these\nword vectors are gradually adjusted via backpropagation, structuring the space into\nsomething the downstream model can exploit. Once fully trained, the embedding\nspace will show a lot of structure—a kind of structure specialized for the specific problem for which you’re training your model.\n Let’s build a model that includes an Embedding layer and benchmark it on our task.\n \n ### Model that uses an `Embedding` layer trained from scratch\n","metadata":{}},{"cell_type":"code","source":"inputs = keras.Input(shape=(None,), dtype=\"int64\")\nembedded = layers.Embedding(input_dim=max_tokens, output_dim=256)(inputs)\nx = layers.Bidirectional(layers.LSTM(32))(embedded)\nx = layers.Dropout(0.5)(x)\noutputs = layers.Dense(1, activation=\"sigmoid\")(x)\nmodel = keras.Model(inputs, outputs)\nmodel.compile(optimizer=\"rmsprop\",\n                loss=\"binary_crossentropy\",\n                metrics=[\"accuracy\"])\nmodel.summary()\ncallbacks = [\n    keras.callbacks.ModelCheckpoint(\"embeddings_bidir_gru.keras\",\n                                    save_best_only=True)\n]\nmodel.fit(int_train_ds, validation_data=int_val_ds, epochs=10,\n        callbacks=callbacks)\nmodel = keras.models.load_model(\"embeddings_bidir_gru.keras\")\nprint(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")","metadata":{"execution":{"iopub.status.busy":"2023-08-10T06:50:14.984946Z","iopub.execute_input":"2023-08-10T06:50:14.986135Z","iopub.status.idle":"2023-08-10T07:03:54.946447Z","shell.execute_reply.started":"2023-08-10T06:50:14.986095Z","shell.execute_reply":"2023-08-10T07:03:54.945275Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Model: \"model_5\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_6 (InputLayer)        [(None, None)]            0         \n                                                                 \n embedding_1 (Embedding)     (None, None, 256)         5120000   \n                                                                 \n bidirectional_1 (Bidirectio  (None, 64)               73984     \n nal)                                                            \n                                                                 \n dropout_5 (Dropout)         (None, 64)                0         \n                                                                 \n dense_9 (Dense)             (None, 1)                 65        \n                                                                 \n=================================================================\nTotal params: 5,194,049\nTrainable params: 5,194,049\nNon-trainable params: 0\n_________________________________________________________________\nEpoch 1/10\n625/625 [==============================] - 114s 171ms/step - loss: 0.5113 - accuracy: 0.7546 - val_loss: 0.3789 - val_accuracy: 0.8444\nEpoch 2/10\n625/625 [==============================] - 69s 110ms/step - loss: 0.3580 - accuracy: 0.8669 - val_loss: 0.3175 - val_accuracy: 0.8754\nEpoch 3/10\n625/625 [==============================] - 54s 87ms/step - loss: 0.2851 - accuracy: 0.8979 - val_loss: 0.3227 - val_accuracy: 0.8712\nEpoch 4/10\n625/625 [==============================] - 49s 79ms/step - loss: 0.2474 - accuracy: 0.9133 - val_loss: 0.3338 - val_accuracy: 0.8742\nEpoch 5/10\n625/625 [==============================] - 46s 74ms/step - loss: 0.2120 - accuracy: 0.9279 - val_loss: 0.3995 - val_accuracy: 0.8732\nEpoch 6/10\n625/625 [==============================] - 45s 71ms/step - loss: 0.1925 - accuracy: 0.9359 - val_loss: 0.3960 - val_accuracy: 0.8766\nEpoch 7/10\n625/625 [==============================] - 40s 64ms/step - loss: 0.1580 - accuracy: 0.9484 - val_loss: 0.3494 - val_accuracy: 0.8820\nEpoch 8/10\n625/625 [==============================] - 42s 66ms/step - loss: 0.1505 - accuracy: 0.9526 - val_loss: 0.3674 - val_accuracy: 0.8846\nEpoch 9/10\n625/625 [==============================] - 41s 66ms/step - loss: 0.1297 - accuracy: 0.9590 - val_loss: 0.3578 - val_accuracy: 0.8828\nEpoch 10/10\n625/625 [==============================] - 42s 67ms/step - loss: 0.1097 - accuracy: 0.9657 - val_loss: 0.3840 - val_accuracy: 0.8784\n782/782 [==============================] - 20s 25ms/step - loss: 0.3438 - accuracy: 0.8594\nTest acc: 0.859\n","output_type":"stream"}]},{"cell_type":"markdown","source":"It trains much faster than the one-hot model (since the LSTM only has to process\n256-dimensional vectors instead of 20,000-dimensional), and its test accuracy is comparable (87%). However, we’re still some way off from the results of our basic bigram\nmodel. Part of the reason why is simply that the model is looking at slightly less data:\nthe bigram model processed full reviews, while our sequence model truncates sequences\nafter 600 words. ","metadata":{}},{"cell_type":"markdown","source":"**UNDERSTANDING PADDING AND MASKING**\n\nOne thing that’s slightly hurting model performance here is that our input sequences\nare full of zeros. This comes from our use of the `output_sequence_length=max_length` option in `TextVectorization` (with `max_length` equal to 600): sentences longer than 600 tokens are truncated to a length of 600 tokens, and sentences shorter\nthan 600 tokens are padded with zeros at the end so that they can be concatenated\ntogether with other sequences to form contiguous batches.\n\nWe’re using a bidirectional RNN: two RNN layers running in parallel, with one\nprocessing the tokens in their natural order, and the other processing the same\ntokens in reverse. The RNN that looks at the tokens in their natural order will spend\nits last iterations seeing only vectors that encode padding—possibly for several hundreds of iterations if the original sentence was short. The information stored in the\ninternal state of the RNN will gradually fade out as it gets exposed to these meaningless inputs.\n\n We need some way to tell the RNN that it should skip these iterations. There’s an\nAPI for that: *masking*.\n\nThe `Embedding` layer is capable of generating a “mask” that corresponds to its\ninput data. This mask is a tensor of ones and zeros (or True/False booleans), of shape\n`(batch_size, sequence_length)`, where the entry `mask[i, t]` indicates where timestep `t` of sample `i` should be skipped or not (the timestep will be skipped if `mask[i, t]`\nis 0 or False, and processed otherwise).\n By default, this option isn’t active—you can turn it on by passing `mask_zero=True`\nto your `Embedding` layer. You can retrieve the mask with the `compute_mask()` method:","metadata":{}},{"cell_type":"code","source":"embedding_layer = layers.Embedding(input_dim=10, output_dim=256, mask_zero=True)\nsome_input = [\n[4, 3, 2, 1, 0, 0, 0],\n[5, 4, 3, 2, 1, 0, 0],\n[2, 1, 0, 0, 0, 0, 0]]\nmask = embedding_layer.compute_mask(some_input)\nmask","metadata":{"execution":{"iopub.status.busy":"2023-08-10T07:09:33.240155Z","iopub.execute_input":"2023-08-10T07:09:33.240604Z","iopub.status.idle":"2023-08-10T07:09:33.262674Z","shell.execute_reply.started":"2023-08-10T07:09:33.240567Z","shell.execute_reply":"2023-08-10T07:09:33.261442Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"<tf.Tensor: shape=(3, 7), dtype=bool, numpy=\narray([[ True,  True,  True,  True, False, False, False],\n       [ True,  True,  True,  True,  True, False, False],\n       [ True,  True, False, False, False, False, False]])>"},"metadata":{}}]},{"cell_type":"markdown","source":"In practice, you will almost never have to manage masks by hand. Instead, Keras will\nautomatically pass on the mask to every layer that is able to process it (as a piece of\nmetadata attached to the sequence it represents). This mask will be used by RNN layers to skip masked steps. If your model returns an entire sequence, the mask will also\nbe used by the loss function to skip masked steps in the output sequence.\n Let’s try retraining our model with masking enabled.\n \n### Using an `Embedding` layer with masking enabled","metadata":{}},{"cell_type":"code","source":"inputs = keras.Input(shape=(None,), dtype=\"int64\")\nembedded = layers.Embedding(input_dim=max_tokens, output_dim=256, mask_zero=True)(inputs)\nx = layers.Bidirectional(layers.LSTM(32))(embedded)\nx = layers.Dropout(0.5)(x)\noutputs = layers.Dense(1, activation=\"sigmoid\")(x)\nmodel = keras.Model(inputs, outputs)\nmodel.compile(optimizer=\"rmsprop\",\n             loss=\"binary_crossentropy\",\n             metrics=[\"accuracy\"])\nmodel.summary()\ncallbacks = [\n    keras.callbacks.ModelCheckpoint(\"embeddings_bidir_gru_with_masking.keras\",\n                                     save_best_only=True)\n]\nmodel.fit(int_train_ds, validation_data=int_val_ds, epochs=10,\n          callbacks=callbacks)\nmodel = keras.models.load_model(\"embeddings_bidir_gru_with_masking.keras\")\nprint(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")","metadata":{"execution":{"iopub.status.busy":"2023-08-10T07:09:36.962235Z","iopub.execute_input":"2023-08-10T07:09:36.962683Z","iopub.status.idle":"2023-08-10T07:23:55.131950Z","shell.execute_reply.started":"2023-08-10T07:09:36.962648Z","shell.execute_reply":"2023-08-10T07:23:55.130630Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Model: \"model_6\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_7 (InputLayer)        [(None, None)]            0         \n                                                                 \n embedding_3 (Embedding)     (None, None, 256)         5120000   \n                                                                 \n bidirectional_2 (Bidirectio  (None, 64)               73984     \n nal)                                                            \n                                                                 \n dropout_6 (Dropout)         (None, 64)                0         \n                                                                 \n dense_10 (Dense)            (None, 1)                 65        \n                                                                 \n=================================================================\nTotal params: 5,194,049\nTrainable params: 5,194,049\nNon-trainable params: 0\n_________________________________________________________________\nEpoch 1/10\n625/625 [==============================] - 104s 149ms/step - loss: 0.4613 - accuracy: 0.7754 - val_loss: 0.4099 - val_accuracy: 0.8010\nEpoch 2/10\n625/625 [==============================] - 60s 95ms/step - loss: 0.2791 - accuracy: 0.8874 - val_loss: 0.2705 - val_accuracy: 0.8874\nEpoch 3/10\n625/625 [==============================] - 52s 83ms/step - loss: 0.2147 - accuracy: 0.9175 - val_loss: 0.2835 - val_accuracy: 0.8880\nEpoch 4/10\n625/625 [==============================] - 48s 77ms/step - loss: 0.1593 - accuracy: 0.9433 - val_loss: 0.3087 - val_accuracy: 0.8744\nEpoch 5/10\n625/625 [==============================] - 47s 76ms/step - loss: 0.1259 - accuracy: 0.9560 - val_loss: 0.3386 - val_accuracy: 0.8854\nEpoch 6/10\n625/625 [==============================] - 46s 73ms/step - loss: 0.0961 - accuracy: 0.9672 - val_loss: 0.3574 - val_accuracy: 0.8800\nEpoch 7/10\n625/625 [==============================] - 42s 67ms/step - loss: 0.0688 - accuracy: 0.9761 - val_loss: 0.4053 - val_accuracy: 0.8794\nEpoch 8/10\n625/625 [==============================] - 42s 67ms/step - loss: 0.0560 - accuracy: 0.9819 - val_loss: 0.5890 - val_accuracy: 0.8476\nEpoch 9/10\n625/625 [==============================] - 44s 70ms/step - loss: 0.0458 - accuracy: 0.9854 - val_loss: 0.6352 - val_accuracy: 0.8604\nEpoch 10/10\n625/625 [==============================] - 42s 67ms/step - loss: 0.0373 - accuracy: 0.9877 - val_loss: 0.5118 - val_accuracy: 0.8734\n782/782 [==============================] - 23s 26ms/step - loss: 0.2955 - accuracy: 0.8753\nTest acc: 0.875\n","output_type":"stream"}]},{"cell_type":"markdown","source":"This time we get to 88% test accuracy—a small but noticeable improvement. ","metadata":{}},{"cell_type":"markdown","source":"**USING PRETRAINED WORD EMBEDDINGS**\n\nSometimes you have so little training data available that you can’t use your data alone\nto learn an appropriate task-specific embedding of your vocabulary. In such cases,\ninstead of learning word embeddings jointly with the problem you want to solve, you\ncan load embedding vectors from a precomputed embedding space that you know is\nhighly structured and exhibits useful properties—one that captures generic aspects of\nlanguage structure. The rationale behind using pretrained word embeddings in natural language processing is much the same as for using pretrained convnets in image\nclassification: you don’t have enough data available to learn truly powerful features on\nyour own, but you expect that the features you need are fairly generic—that is, common visual features or semantic features. In this case, it makes sense to reuse features\nlearned on a different problem.\n\nSuch word embeddings are generally computed using *word-occurrence statistics*\n(observations about what words co-occur in sentences or documents), using a variety of\ntechniques, some involving neural networks, others not. The idea of a dense, lowdimensional embedding space for words, computed in an unsupervised way, was initially explored by Bengio et al. in the early 2000s (see Yoshua Bengio et al., Neural Probabilistic Language Models (Springer, 2003)),\n but it only started to take off in\nresearch and industry applications after the release of one of the most famous and successful word-embedding schemes: the Word2vec algorithm (https://code.google.com/archive/p/word2vec), developed by Tomas Mikolov at Google in 2013. Word2vec\ndimensions capture specific semantic properties, such as gender.\n\nThere are various precomputed databases of word embeddings that you can download and use in a Keras `Embedding` layer. Word2vec is one of them. Another popular\none is called Global Vectors for Word Representation (GloVe, https://nlp.stanford\n.edu/projects/glove), which was developed by Stanford researchers in 2014. This\nembedding technique is based on factorizing a matrix of word co-occurrence statistics. Its developers have made available precomputed embeddings for millions of\nEnglish tokens, obtained from Wikipedia data and Common Crawl data.\n\nLet’s look at how you can get started using GloVe embeddings in a Keras model.\nThe same method is valid for Word2Vec embeddings or any other word-embedding\ndatabase. We’ll start by downloading the GloVe files and parse them. We’ll then load\nthe word vectors into a Keras Embedding layer, which we’ll use to build a new model.\n\nFirst, let’s download the GloVe word embeddings precomputed on the 2014\nEnglish Wikipedia dataset. It’s an 822 MB zip file containing 100-dimensional embedding vectors for 400,000 words (or non-word tokens).","metadata":{}},{"cell_type":"code","source":"!wget http://nlp.stanford.edu/data/glove.6B.zip\n!unzip -q glove.6B.zip","metadata":{"execution":{"iopub.status.busy":"2023-08-10T07:41:47.429281Z","iopub.execute_input":"2023-08-10T07:41:47.429734Z","iopub.status.idle":"2023-08-10T07:45:43.475819Z","shell.execute_reply.started":"2023-08-10T07:41:47.429695Z","shell.execute_reply":"2023-08-10T07:45:43.474152Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"--2023-08-10 07:41:48--  http://nlp.stanford.edu/data/glove.6B.zip\nResolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\nConnecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://nlp.stanford.edu/data/glove.6B.zip [following]\n--2023-08-10 07:41:48--  https://nlp.stanford.edu/data/glove.6B.zip\nConnecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\nHTTP request sent, awaiting response... 301 Moved Permanently\nLocation: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n--2023-08-10 07:41:49--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\nResolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\nConnecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 862182613 (822M) [application/zip]\nSaving to: ‘glove.6B.zip’\n\nglove.6B.zip        100%[===================>] 822.24M  4.90MB/s    in 3m 28s  \n\n2023-08-10 07:45:17 (3.95 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Let’s parse the unzipped file (a .txt file) to build an index that maps words (as strings)\nto their vector representation.\n\n### Parsing the GloVe word-embeddings file","metadata":{}},{"cell_type":"code","source":"import numpy as np\npath_to_glove_file = \"glove.6B.100d.txt\"\n\nembeddings_index = {}\nwith open(path_to_glove_file) as f:\n    for line in f:\n        word, coefs = line.split(maxsplit=1)\n        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n        embeddings_index[word] = coefs\nprint(f\"Found {len(embeddings_index)} word vectors.\")","metadata":{"execution":{"iopub.status.busy":"2023-08-10T07:50:21.514765Z","iopub.execute_input":"2023-08-10T07:50:21.516136Z","iopub.status.idle":"2023-08-10T07:50:37.261862Z","shell.execute_reply.started":"2023-08-10T07:50:21.516086Z","shell.execute_reply":"2023-08-10T07:50:37.260738Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Found 400000 word vectors.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Next, let’s build an embedding matrix that you can load into an `Embedding` layer. It\nmust be a matrix of shape `(max_words, embedding_dim)`, where each entry *i* contains\nthe `embedding_dim`-dimensional vector for the word of index *i* in the reference word\nindex (built during tokenization).\n\n### Preparing the GloVe word-embeddings matrix","metadata":{}},{"cell_type":"code","source":"embedding_dim = 100\n\n# Retrieve the vocabulary indexed by our previous TextVectorization layer.\nvocabulary = text_vectorization.get_vocabulary()\n# Use it to create a mapping from words to their index in the vocabulary.\nword_index = dict(zip(vocabulary, range(len(vocabulary))))\n\n# Prepare a matrix that we’ll fill with the GloVe vectors.\nembedding_matrix = np.zeros((max_tokens, embedding_dim))\nfor word, i in word_index.items():\n    if i < max_tokens:\n        embedding_vector = embeddings_index.get(word)\n    # Fill entry i in the matrix with the word vector for index i. \n    # Words not found in the embedding index will be all zeros.\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector ","metadata":{"execution":{"iopub.status.busy":"2023-08-10T07:51:34.395026Z","iopub.execute_input":"2023-08-10T07:51:34.396455Z","iopub.status.idle":"2023-08-10T07:51:34.528817Z","shell.execute_reply.started":"2023-08-10T07:51:34.396393Z","shell.execute_reply":"2023-08-10T07:51:34.527734Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"Finally, we use a `Constant` initializer to load the pretrained embeddings in an Embedding\nlayer. So as not to disrupt the pretrained representations during training, we freeze\nthe layer via `trainable=False`:","metadata":{}},{"cell_type":"code","source":"embedding_layer = layers.Embedding(\n    max_tokens,\n    embedding_dim,\n    embeddings_initializer=keras.initializers.Constant(embedding_matrix), \n    trainable=False,\n    mask_zero=True,\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T07:52:49.887472Z","iopub.execute_input":"2023-08-10T07:52:49.888589Z","iopub.status.idle":"2023-08-10T07:52:49.902365Z","shell.execute_reply.started":"2023-08-10T07:52:49.888554Z","shell.execute_reply":"2023-08-10T07:52:49.901081Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"We’re now ready to train a new model—identical to our previous model, but leveraging the 100-dimensional pretrained GloVe embeddings instead of 128-dimensional\nlearned embeddings.\n### Model that uses a pretrained Embedding layer","metadata":{}},{"cell_type":"code","source":"inputs = keras.Input(shape=(None,), dtype=\"int64\")\nembedded = embedding_layer(inputs)\nx = layers.Bidirectional(layers.LSTM(32))(embedded)\nx = layers.Dropout(0.5)(x)\noutputs = layers.Dense(1, activation=\"sigmoid\")(x)\nmodel = keras.Model(inputs, outputs)\nmodel.compile(optimizer=\"rmsprop\",\n             loss=\"binary_crossentropy\",\n             metrics=[\"accuracy\"])\nmodel.summary()\ncallbacks = [\n    keras.callbacks.ModelCheckpoint(\"glove_embeddings_sequence_model.keras\",\n                                    save_best_only=True)\n]\nmodel.fit(int_train_ds, validation_data=int_val_ds, epochs=10,\n         callbacks=callbacks)\nmodel = keras.models.load_model(\"glove_embeddings_sequence_model.keras\")\nprint(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")","metadata":{"execution":{"iopub.status.busy":"2023-08-10T07:53:50.857218Z","iopub.execute_input":"2023-08-10T07:53:50.857675Z","iopub.status.idle":"2023-08-10T08:05:16.191267Z","shell.execute_reply.started":"2023-08-10T07:53:50.857641Z","shell.execute_reply":"2023-08-10T08:05:16.189781Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"Model: \"model_7\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_8 (InputLayer)        [(None, None)]            0         \n                                                                 \n embedding_4 (Embedding)     (None, None, 100)         2000000   \n                                                                 \n bidirectional_3 (Bidirectio  (None, 64)               34048     \n nal)                                                            \n                                                                 \n dropout_7 (Dropout)         (None, 64)                0         \n                                                                 \n dense_11 (Dense)            (None, 1)                 65        \n                                                                 \n=================================================================\nTotal params: 2,034,113\nTrainable params: 34,113\nNon-trainable params: 2,000,000\n_________________________________________________________________\nEpoch 1/10\n625/625 [==============================] - 56s 73ms/step - loss: 0.5874 - accuracy: 0.6848 - val_loss: 0.5067 - val_accuracy: 0.7626\nEpoch 2/10\n625/625 [==============================] - 41s 66ms/step - loss: 0.4649 - accuracy: 0.7854 - val_loss: 0.3890 - val_accuracy: 0.8316\nEpoch 3/10\n625/625 [==============================] - 41s 66ms/step - loss: 0.4074 - accuracy: 0.8207 - val_loss: 0.3772 - val_accuracy: 0.8338\nEpoch 4/10\n625/625 [==============================] - 42s 68ms/step - loss: 0.3745 - accuracy: 0.8371 - val_loss: 0.3434 - val_accuracy: 0.8530\nEpoch 5/10\n625/625 [==============================] - 43s 69ms/step - loss: 0.3504 - accuracy: 0.8510 - val_loss: 0.3157 - val_accuracy: 0.8722\nEpoch 6/10\n625/625 [==============================] - 45s 71ms/step - loss: 0.3255 - accuracy: 0.8637 - val_loss: 0.3020 - val_accuracy: 0.8750\nEpoch 7/10\n625/625 [==============================] - 41s 66ms/step - loss: 0.3068 - accuracy: 0.8731 - val_loss: 0.2910 - val_accuracy: 0.8800\nEpoch 8/10\n625/625 [==============================] - 43s 69ms/step - loss: 0.2887 - accuracy: 0.8834 - val_loss: 0.2907 - val_accuracy: 0.8880\nEpoch 9/10\n625/625 [==============================] - 39s 63ms/step - loss: 0.2755 - accuracy: 0.8907 - val_loss: 0.2917 - val_accuracy: 0.8816\nEpoch 10/10\n625/625 [==============================] - 41s 66ms/step - loss: 0.2602 - accuracy: 0.8962 - val_loss: 0.2838 - val_accuracy: 0.8834\n782/782 [==============================] - 23s 26ms/step - loss: 0.2909 - accuracy: 0.8771\nTest acc: 0.877\n","output_type":"stream"}]},{"cell_type":"markdown","source":"You’ll find that on this particular task, pretrained embeddings aren’t very helpful,\nbecause the dataset contains enough samples that it is possible to learn a specialized\nenough embedding space from scratch. However, leveraging pretrained embeddings\ncan be very helpful when you’re working with a smaller dataset. ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}