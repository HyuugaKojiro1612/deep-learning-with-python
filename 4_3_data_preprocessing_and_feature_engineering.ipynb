{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13fe8bd5",
   "metadata": {},
   "source": [
    "# 4.3 Data preprocessing, feature engineering, and feature learning\n",
    "\n",
    "In addition to model evaluation, an important question we must tackle before we dive deeper into model development is the following: how do you prepare the input data\n",
    "and targets before feeding them into a neural network? Many data-preprocessing and feature-engineering techniques are domain specific (for example, specific to text data or image data); we’ll cover those in the following chapters as we encounter them in practical examples. For now, we’ll review the basics that are common to all data domains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380581ef",
   "metadata": {},
   "source": [
    "## 4.3.1 Data preprocessing for neural networks\n",
    "\n",
    "Data preprocessing aims at making the raw data at hand more amenable to neural networks. This includes vectorization, normalization, handling missing values, and feature extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfec7b4d",
   "metadata": {},
   "source": [
    "### *Vectorization*\n",
    "\n",
    "All inputs and targets in a neural network must be tensors of floating-point data (or, in specific cases, tensors of integers). Whatever data you need to process—sound, images, text—you must first turn into tensors, a step called *data vectorization*. For instance, in the two previous text-classification examples, we started from text represented as lists of integers (standing for sequences of words), and we used one-hot encoding to turn them into a tensor of `float32` data. In the examples of classifying digits and predicting house prices, the data already came in vectorized form, so you were able to skip this step. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d58d39",
   "metadata": {},
   "source": [
    "*Movie reviews classification example*\n",
    "\n",
    "The variables `train_data` and `test_data` are lists of reviews; each review is a list of word indices (encoding a sequence of words). `train_labels` and `test_labels` are lists of 0s and 1s, where 0 stands for negative and 1 stands for positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "690350fa",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of sequence: 218\n",
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
      "label: 1\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "\n",
    "# Keeps the top 10000 most frequently occurring words in the training data\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n",
    "\n",
    "# Explores a sample in training set\n",
    "print(\"length of sequence:\", len(train_data[0]))\n",
    "print(train_data[0])\n",
    "print(\"label:\", train_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac9501c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "[0. 1. 1. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Encoding the integer sequences into a binary matrix\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results\n",
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)\n",
    "\n",
    "print(len(x_train[0]))\n",
    "print(x_train[0])\n",
    "\n",
    "\n",
    "y_train = np.asarray(train_labels).astype('float32')\n",
    "y_test = np.asarray(test_labels).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49e1aac",
   "metadata": {},
   "source": [
    "*Newswires classification example*\n",
    "\n",
    "To vectorize the labels, there are two possibilities: you can cast the label list as an integer tensor, or you can use one-hot encoding. One-hot encoding is a widely used format for categorical data, also called *categorical encoding*. For a more detailed explanation of one-hot encoding, see section 6.1. In this case, one-hot encoding of the labels consists of embedding each label as an all-zero vector with a 1 in the place of the label index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f17d407",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import reuters\n",
    "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(\n",
    "num_words=10000)\n",
    "\n",
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)\n",
    "\n",
    "# Vectorizes the labels\n",
    "from keras.utils.np_utils import to_categorical\n",
    "one_hot_train_labels = to_categorical(train_labels)\n",
    "one_hot_test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9583b6a9",
   "metadata": {},
   "source": [
    "### *Value normalization*\n",
    "\n",
    "In the digit-classification example, you started from image data encoded as integers in the 0–255 range, encoding grayscale values. Before you fed this data into your network, you had to cast it to `float32` and divide by 255 so you’d end up with floatingpoint values in the 0–1 range. Similarly, when predicting house prices, you started from features that took a variety of ranges—some features had small floating-point values, others had fairly large integer values. Before you fed this data into your network, you had to normalize each feature independently so that it had a standard deviation of 1 and a mean of 0.\n",
    "\n",
    "In general, it isn’t safe to feed into a neural network data that takes relatively large values (for example, multidigit integers, which are much larger than the initial values taken by the weights of a network) or data that is heterogeneous (for example, data where one feature is in the range 0–1 and another is in the range 100–200). Doing so can trigger large gradient updates that will prevent the network from converging. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "To make learning easier for your network, your data should have the following characteristics:\n",
    "    <ul>\n",
    "        <li> <i>Take small values</i>—Typically, most values should be in the 0–1 range.</li>\n",
    "        <li> <i>Be homogenous</i>—That is, all features should take values in roughly the same range.</li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "Additionally, the following stricter normalization practice is common and can help, although it isn’t always necessary (for example, you didn’t do this in the digit-classification example):\n",
    "- Normalize each feature independently to have a mean of 0.\n",
    "- Normalize each feature independently to have a standard deviation of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f822e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming x is a 2D data matrix of shape (samples, features)\n",
    "x -= x.mean(axis=0)\n",
    "x /= x.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b06a390",
   "metadata": {},
   "source": [
    "### *Handling missing values*\n",
    "\n",
    "You may sometimes have missing values in your data. For instance, in the house-price example, the first feature (the column of index 0 in the data) was the per capita crime rate. What if this feature wasn’t available for all samples? You’d then have missing values in the training or test data.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "In general, with neural networks, it’s safe to input missing values as 0, with the condition that 0 isn’t already a meaningful value. The network will learn from exposure to the data that the value 0 means <i>missing data</i> and will start ignoring the value.\n",
    "</div>\n",
    " \n",
    " Note that if you’re expecting missing values in the test data, but the network was trained on data without any missing values, the network won’t have learned to ignore missing values! \n",
    " \n",
    "In this situation, you should artificially generate training samples with missing entries: copy some training samples several times, and drop some of the features that you expect are likely to be missing in the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fb204f",
   "metadata": {},
   "source": [
    "## 4.3.2 Feature engineering\n",
    "\n",
    "*Feature engineering* is the process of using your own knowledge about the data and about the machine-learning algorithm at hand (in this case, a neural network) to make the algorithm work better by applying hardcoded (nonlearned) transformations to the data before it goes into the model. In many cases, it isn’t reasonable to expect a machinelearning model to be able to learn from completely arbitrary data. The data needs to be presented to the model in a way that will make the model’s job easier.\n",
    "\n",
    "The essence of feature engineering is making a problem easier by expressing it in a simpler way. It usually requires understanding the problem in depth.\n",
    "\n",
    "<img src=\"https://www.bpesquet.fr/mlhandbook/_images/feature_engineering.png\" \n",
    "     align=\"center\" \n",
    "     width=\"400\" />\n",
    " \n",
    " Before deep learning, feature engineering used to be critical, because classical shallow algorithms didn’t have hypothesis spaces rich enough to learn useful features by themselves. The way you presented the data to the algorithm was essential to its success. For instance, before convolutional neural networks became successful on the MNIST digit-classification problem, solutions were typically based on hardcoded features such as the number of loops in a digit image, the height of each digit in an image, a histogram of pixel values, and so on.\n",
    "\n",
    "Fortunately, modern deep learning removes the need for most feature engineering, because neural networks are capable of automatically extracting useful features from raw data. Does this mean you don’t have to worry about feature engineering as long as you’re using deep neural networks? No, for two reasons:\n",
    "- Good features still allow you to solve problems more elegantly while using fewer resources. For instance, it would be ridiculous to solve the problem of reading a clock face using a convolutional neural network.\n",
    "- Good features let you solve a problem with far less data. The ability of deeplearning models to learn features on their own relies on having lots of training data available; if you have only a few samples, then the information value in their features becomes critical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fee746",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
