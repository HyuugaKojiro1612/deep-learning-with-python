{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 11.3 Two approaches for representing groups of words: Sets and sequences","metadata":{}},{"cell_type":"markdown","source":"How a machine learning model should represent individual words is a relatively uncontroversial question: they’re categorical features (values from a predefined set), and we\nknow how to handle those. They should be encoded as dimensions in a feature space,\nor as category vectors (word vectors in this case). A much more problematic question,\nhowever, is how to encode *the way words are woven into sentences*: word order.\n\nThe problem of order in natural language is an interesting one: unlike the steps of\na timeseries, words in a sentence don’t have a natural, canonical order. Different languages order similar words in very different ways. For instance, the sentence structure\nof English is quite different from that of Japanese. Even within a given language, you\ncan typically say the same thing in different ways by reshuffling the words a bit. Even\nfurther, if you fully randomize the words in a short sentence, you can still largely figure out what it was saying—though in many cases significant ambiguity seems to arise.\nOrder is clearly important, but its relationship to meaning isn’t straightforward.\n\n How to represent word order is the pivotal question from which different kinds of\nNLP architectures spring. The simplest thing you could do is just discard order and\ntreat text as an unordered set of words—this gives you *bag-of-words models*. You could\nalso decide that words should be processed strictly in the order in which they appear,\none at a time, like steps in a timeseries—you could then leverage the recurrent models\nfrom the last chapter. Finally, a hybrid approach is also possible: the Transformer architecture is technically order-agnostic, yet it injects word-position information into\nthe representations it processes, which enables it to simultaneously look at different\nparts of a sentence (unlike RNNs) while still being order-aware. Because they take into\naccount word order, both RNNs and Transformers are called *sequence models*.\n\nHistorically, most early applications of machine learning to NLP just involved\nbag-of-words models. Interest in sequence models only started rising in 2015, with the\nrebirth of recurrent neural networks. Today, both approaches remain relevant. Let’s\nsee how they work, and when to leverage which.\n\nWe’ll demonstrate each approach on a well-known text classification benchmark:\nthe IMDB movie review sentiment-classification dataset. In chapters 4 and 5, you\nworked with a prevectorized version of the IMDB dataset; now, let’s process the raw\nIMDB text data, just like you would do when approaching a new text-classification\nproblem in the real world.","metadata":{}},{"cell_type":"markdown","source":"## 11.3.1 Preparing the IMDB movie reviews data\n\nLet’s start by downloading the dataset from the Stanford page of Andrew Maas and\nuncompressing it:","metadata":{}},{"cell_type":"code","source":"!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n!tar -xf aclImdb_v1.tar.gz\n\n!rm -r aclImdb/train/unsup","metadata":{"execution":{"iopub.status.busy":"2023-08-09T17:46:49.541734Z","iopub.execute_input":"2023-08-09T17:46:49.542332Z","iopub.status.idle":"2023-08-09T17:47:08.688532Z","shell.execute_reply.started":"2023-08-09T17:46:49.542295Z","shell.execute_reply":"2023-08-09T17:47:08.687196Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"curl: /opt/conda/lib/libcurl.so.4: no version information available (required by curl)\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100 80.2M  100 80.2M    0     0  9282k      0  0:00:08  0:00:08 --:--:-- 16.4M\n","output_type":"stream"}]},{"cell_type":"markdown","source":"You’re left with a directory named aclImdb. The train/pos/ directory contains a set of 12,500 text files, each of which\ncontains the text body of a positive-sentiment movie review to be used as training data.\nThe negative-sentiment reviews live in the “neg” directories. In total, there are 25,000\ntext files for training and another 25,000 for testing.\n\nTake a look at the content of a few of these text files. Whether you’re working with\ntext data or image data, remember to always inspect what your data looks like before\nyou dive into modeling it. It will ground your intuition about what your model is actually doing:","metadata":{}},{"cell_type":"code","source":"!cat aclImdb/train/pos/4077_10.txt","metadata":{"execution":{"iopub.status.busy":"2023-08-09T17:47:16.385163Z","iopub.execute_input":"2023-08-09T17:47:16.385579Z","iopub.status.idle":"2023-08-09T17:47:17.370990Z","shell.execute_reply.started":"2023-08-09T17:47:16.385545Z","shell.execute_reply":"2023-08-09T17:47:17.369811Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"I first saw this back in the early 90s on UK TV, i did like it then but i missed the chance to tape it, many years passed but the film always stuck with me and i lost hope of seeing it TV again, the main thing that stuck with me was the end, the hole castle part really touched me, its easy to watch, has a great story, great music, the list goes on and on, its OK me saying how good it is but everyone will take there own best bits away with them once they have seen it, yes the animation is top notch and beautiful to watch, it does show its age in a very few parts but that has now become part of it beauty, i am so glad it has came out on DVD as it is one of my top 10 films of all time. Buy it or rent it just see it, best viewing is at night alone with drink and food in reach so you don't have to stop the film.<br /><br />Enjoy","output_type":"stream"}]},{"cell_type":"markdown","source":"Next, let’s prepare a validation set by setting apart 20% of the training text files in a\nnew directory, aclImdb/val:","metadata":{}},{"cell_type":"code","source":"import os, pathlib, shutil, random\n\nbase_dir = pathlib.Path(\"aclImdb\")\nval_dir = base_dir / \"val\"\ntrain_dir = base_dir / \"train\"\nfor category in (\"neg\", \"pos\"):\n    os.makedirs(val_dir / category)\n    files = os.listdir(train_dir / category)\n    # Shuffle the list of training files using a seed, to ensure we get the same validation set every time we run the code.\n    random.Random(1337).shuffle(files)\n    # Take 20% of the training files to use for validation.\n    num_val_samples = int(0.2 * len(files))\n    val_files = files[-num_val_samples:]\n    # Move the files to aclImdb/val/neg and aclImdb/val/pos.\n    for fname in val_files:\n        shutil.move(train_dir / category / fname, val_dir / category / fname)","metadata":{"execution":{"iopub.status.busy":"2023-08-09T17:47:20.154956Z","iopub.execute_input":"2023-08-09T17:47:20.155420Z","iopub.status.idle":"2023-08-09T17:47:20.559563Z","shell.execute_reply.started":"2023-08-09T17:47:20.155377Z","shell.execute_reply":"2023-08-09T17:47:20.558524Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"Remember how, in chapter 8, we used the `image_dataset_from_directory` utility to\ncreate a batched `Dataset` of images and their labels for a directory structure? You can\ndo the exact same thing for text files using the `text_dataset_from_directory` utility.\nLet’s create three Dataset objects for training, validation, and testing:","metadata":{}},{"cell_type":"code","source":"from tensorflow import keras","metadata":{"execution":{"iopub.status.busy":"2023-08-09T17:47:33.549569Z","iopub.execute_input":"2023-08-09T17:47:33.550293Z","iopub.status.idle":"2023-08-09T17:47:33.554850Z","shell.execute_reply.started":"2023-08-09T17:47:33.550256Z","shell.execute_reply":"2023-08-09T17:47:33.553730Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"Running `train_ds = ...` line should\noutput “Found 20000 files\nbelonging to 2 classes”;\nif you see “Found 70000\nfiles belonging to 3\nclasses,” it means you\nforgot to delete the\naclImdb/train/unsup\ndirectory.","metadata":{}},{"cell_type":"code","source":"batch_size = 32\n\ntrain_ds = keras.utils.text_dataset_from_directory(\n    \"aclImdb/train\", batch_size=batch_size\n)\nval_ds = keras.utils.text_dataset_from_directory(\n    \"aclImdb/val\", batch_size=batch_size\n)\ntest_ds = keras.utils.text_dataset_from_directory(\n    \"aclImdb/test\", batch_size=batch_size\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-09T17:47:36.357368Z","iopub.execute_input":"2023-08-09T17:47:36.357728Z","iopub.status.idle":"2023-08-09T17:47:43.104382Z","shell.execute_reply.started":"2023-08-09T17:47:36.357699Z","shell.execute_reply":"2023-08-09T17:47:43.103344Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Found 20000 files belonging to 2 classes.\nFound 5000 files belonging to 2 classes.\nFound 25000 files belonging to 2 classes.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"These datasets yield inputs that are TensorFlow `tf.string` tensors and targets that are `int32` tensors encoding the value “0” or “1.”\n\n### Displaying the shapes and dtypes of the first batch","metadata":{}},{"cell_type":"code","source":"for inputs, targets in train_ds:\n    print(\"inputs.shape:\", inputs.shape)\n    print(\"inputs.dtype:\", inputs.dtype)\n    print(\"targets.shape:\", targets.shape)\n    print(\"targets.dtype:\", targets.dtype)\n    print(\"inputs[0]:\", inputs[0])\n    print(\"targets[0]:\", targets[0])\n    break","metadata":{"execution":{"iopub.status.busy":"2023-08-09T17:47:46.310847Z","iopub.execute_input":"2023-08-09T17:47:46.311247Z","iopub.status.idle":"2023-08-09T17:47:46.401611Z","shell.execute_reply.started":"2023-08-09T17:47:46.311217Z","shell.execute_reply":"2023-08-09T17:47:46.400660Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"inputs.shape: (32,)\ninputs.dtype: <dtype: 'string'>\ntargets.shape: (32,)\ntargets.dtype: <dtype: 'int32'>\ninputs[0]: tf.Tensor(b\"This was an absolutely spellbinding series and was sorry that I was only able to catch a few shows way back when it aired late night in the UK. The style of it was so different from others of its kind and the whole thing had an unnerving air of stylish dread to it. All you have to do is read all the positive comments (not a single negative that I can see) to realise what a really innovative series this was and how it caught at the imagination. I now understand from reading the comments it got CANCELLED that's just so unbelievable. What a bunch of 'headless overpaid suited turkeys' there must have been (or just maybe still are) running around to do that.\", shape=(), dtype=string)\ntargets[0]: tf.Tensor(1, shape=(), dtype=int32)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 11.3.2 Processing words as a set: The bag-of-words approach\n\nThe simplest way to encode a piece of text for processing by a machine learning\nmodel is to discard order and treat it as a set (a “bag”) of tokens. You could either look\nat individual words (unigrams), or try to recover some local order information by\nlooking at groups of consecutive token (N-grams).\n\n**SINGLE WORDS (UNIGRAMS) WITH BINARY ENCODING**\n\nIf you use a bag of single words, the sentence “the cat sat on the mat” becomes\n```\n{\"cat\", \"mat\", \"on\", \"sat\", \"the\"}\n```\nThe main advantage of this encoding is that you can represent an entire text as a single vector, where each entry is a presence indicator for a given word. For instance,\nusing binary encoding (multi-hot), you’d encode a text as a vector with as many\ndimensions as there are words in your vocabulary—with 0s almost everywhere and\nsome 1s for dimensions that encode words present in the text. This is what we did\nwhen we worked with text data in chapters 4 and 5. Let’s try this on our task.\n\nFirst, let’s process our raw text datasets with a `TextVectorization` layer so that\nthey yield multi-hot encoded binary word vectors. Our layer will only look at single\nwords (that is to say, unigrams).\n\n### Preprocessing our datasets with a `TextVectorization` layer\n\nLimit the vocabulary to the 20,000 most frequent words.\nOtherwise we’d be indexing every word in the training data—\npotentially tens of thousands of terms that only occur once or\ntwice and thus aren’t informative. In general, 20,000 is the\nright vocabulary size for text classification.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.layers import TextVectorization\n\ntext_vectorization = TextVectorization(\n    max_tokens=20000,\n    # Encode the output tokens as multi-hot binary vectors.\n    output_mode=\"multi_hot\",\n)\n\n# Prepare a dataset that only yields raw text inputs (no labels).\ntext_only_train_ds = train_ds.map(lambda x, y: x)\n# Use that dataset to index the dataset vocabulary via the adapt() method.\ntext_vectorization.adapt(text_only_train_ds)\n\n# Prepare processed versions of our training, validation, and test dataset.\n# Make sure to specify num_parallel_calls to leverage multiple CPU cores.\nbinary_1gram_train_ds = train_ds.map(\n    lambda x, y: (text_vectorization(x), y),\n    num_parallel_calls=4\n)\nbinary_1gram_val_ds = val_ds.map(\n    lambda x, y: (text_vectorization(x), y),\n    num_parallel_calls=4\n)\nbinary_1gram_test_ds = test_ds.map(\n    lambda x, y: (text_vectorization(x), y),\n    num_parallel_calls=4\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-09T17:47:52.152804Z","iopub.execute_input":"2023-08-09T17:47:52.153484Z","iopub.status.idle":"2023-08-09T17:47:57.825788Z","shell.execute_reply.started":"2023-08-09T17:47:52.153449Z","shell.execute_reply":"2023-08-09T17:47:57.824744Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"for inputs, targets in binary_1gram_train_ds:\n    print(\"inputs.shape:\", inputs.shape)\n    print(\"inputs.dtype:\", inputs.dtype)\n    print(\"targets.shape:\", targets.shape)\n    print(\"targets.dtype:\", targets.dtype)\n    print(\"inputs[0]:\", inputs[0])\n    print(\"targets[0]:\", targets[0])\n    break","metadata":{"execution":{"iopub.status.busy":"2023-08-09T17:48:03.454058Z","iopub.execute_input":"2023-08-09T17:48:03.454959Z","iopub.status.idle":"2023-08-09T17:48:03.572371Z","shell.execute_reply.started":"2023-08-09T17:48:03.454889Z","shell.execute_reply":"2023-08-09T17:48:03.571277Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"inputs.shape: (32, 20000)\ninputs.dtype: <dtype: 'float32'>\ntargets.shape: (32,)\ntargets.dtype: <dtype: 'int32'>\ninputs[0]: tf.Tensor([1. 1. 1. ... 0. 0. 0.], shape=(20000,), dtype=float32)\ntargets[0]: tf.Tensor(0, shape=(), dtype=int32)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Next, let’s write a reusable model-building function that we’ll use in all of our experiments in this section.\n\n### Our model-building utility","metadata":{}},{"cell_type":"code","source":"from tensorflow import keras\nfrom tensorflow.keras import layers\n\ndef get_model(max_tokens=20000, hidden_dim=16):\n    inputs = keras.Input(shape=(max_tokens,))\n    x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n    x = layers.Dropout(0.5)(x)\n    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n    model = keras.Model(inputs, outputs)\n    model.compile(optimizer=\"rmsprop\", \n                  loss=\"binary_crossentropy\",\n                  metrics=[\"accuracy\"])\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-08-09T17:48:07.368063Z","iopub.execute_input":"2023-08-09T17:48:07.368428Z","iopub.status.idle":"2023-08-09T17:48:07.375818Z","shell.execute_reply.started":"2023-08-09T17:48:07.368397Z","shell.execute_reply":"2023-08-09T17:48:07.374665Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"### Training and testing the binary unigram model\n\nIn `model.fit()`, we call `cache()` on the\ndatasets to cache them in\nmemory: this way, we will\nonly do the preprocessing\nonce, during the first\nepoch, and we’ll reuse the\npreprocessed texts for the\nfollowing epochs. This can\nonly be done if the data\nis small enough to fit in\nmemory.","metadata":{}},{"cell_type":"code","source":"model = get_model()\nmodel.summary()\ncallbacks = [\n    keras.callbacks.ModelCheckpoint(\"binary_1gram.keras\",\n                                   save_best_only=True)\n]\nmodel.fit(binary_1gram_train_ds.cache(), \n         validation_data=binary_1gram_val_ds.cache(),\n         epochs=10,\n         callbacks=callbacks)\nmodel = keras.models.load_model(\"binary_1gram.keras\")\nprint(f\"Test acc: {model.evaluate(binary_1gram_test_ds)[1]:.3f}\")","metadata":{"execution":{"iopub.status.busy":"2023-08-09T17:48:11.529728Z","iopub.execute_input":"2023-08-09T17:48:11.530194Z","iopub.status.idle":"2023-08-09T17:48:51.403579Z","shell.execute_reply.started":"2023-08-09T17:48:11.530158Z","shell.execute_reply":"2023-08-09T17:48:51.402226Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Model: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 20000)]           0         \n                                                                 \n dense (Dense)               (None, 16)                320016    \n                                                                 \n dropout (Dropout)           (None, 16)                0         \n                                                                 \n dense_1 (Dense)             (None, 1)                 17        \n                                                                 \n=================================================================\nTotal params: 320,033\nTrainable params: 320,033\nNon-trainable params: 0\n_________________________________________________________________\nEpoch 1/10\n625/625 [==============================] - 9s 9ms/step - loss: 0.4240 - accuracy: 0.8215 - val_loss: 0.2917 - val_accuracy: 0.8806\nEpoch 2/10\n625/625 [==============================] - 3s 4ms/step - loss: 0.2862 - accuracy: 0.8963 - val_loss: 0.2785 - val_accuracy: 0.8788\nEpoch 3/10\n625/625 [==============================] - 3s 4ms/step - loss: 0.2516 - accuracy: 0.9100 - val_loss: 0.2882 - val_accuracy: 0.8786\nEpoch 4/10\n625/625 [==============================] - 3s 4ms/step - loss: 0.2346 - accuracy: 0.9199 - val_loss: 0.2992 - val_accuracy: 0.8816\nEpoch 5/10\n625/625 [==============================] - 3s 4ms/step - loss: 0.2259 - accuracy: 0.9247 - val_loss: 0.3100 - val_accuracy: 0.8776\nEpoch 6/10\n625/625 [==============================] - 3s 4ms/step - loss: 0.2161 - accuracy: 0.9276 - val_loss: 0.3262 - val_accuracy: 0.8802\nEpoch 7/10\n625/625 [==============================] - 3s 4ms/step - loss: 0.2112 - accuracy: 0.9328 - val_loss: 0.3364 - val_accuracy: 0.8756\nEpoch 8/10\n625/625 [==============================] - 3s 5ms/step - loss: 0.2116 - accuracy: 0.9329 - val_loss: 0.3441 - val_accuracy: 0.8736\nEpoch 9/10\n625/625 [==============================] - 3s 4ms/step - loss: 0.2009 - accuracy: 0.9370 - val_loss: 0.3550 - val_accuracy: 0.8718\nEpoch 10/10\n625/625 [==============================] - 3s 5ms/step - loss: 0.1989 - accuracy: 0.9389 - val_loss: 0.3554 - val_accuracy: 0.8742\n782/782 [==============================] - 4s 5ms/step - loss: 0.2930 - accuracy: 0.8813\nTest acc: 0.881\n","output_type":"stream"}]},{"cell_type":"markdown","source":"This gets us to a test accuracy of 88.7%: not bad! Note that in this case, since the dataset is a balanced two-class classification dataset (there are as many positive samples as\nnegative samples), the “naive baseline” we could reach without training an actual model\nwould only be 50%. Meanwhile, the best score that can be achieved on this dataset\nwithout leveraging external data is around 95% test accuracy. ","metadata":{}},{"cell_type":"markdown","source":"**BIGRAMS WITH BINARY ENCODING**\n\nOf course, discarding word order is very reductive, because even atomic concepts can\nbe expressed via multiple words: the term “United States” conveys a concept that is\nquite distinct from the meaning of the words “states” and “united” taken separately.\nFor this reason, you will usually end up re-injecting local order information into your\nbag-of-words representation by looking at N-grams rather than single words (most\ncommonly, bigrams).\n With bigrams, our sentence becomes\n```\n{\"the\", \"the cat\", \"cat\", \"cat sat\", \"sat\",\n \"sat on\", \"on\", \"on the\", \"the mat\", \"mat\"}\n```\nThe `TextVectorization` layer can be configured to return arbitrary N-grams: bigrams,\ntrigrams, etc. Just pass an `ngrams=N` argument as in the following listing.\n\n### Configuring the `TextVectorization` layer to return bigrams","metadata":{}},{"cell_type":"code","source":"text_vectorization = TextVectorization(\n    ngrams=2,\n    max_tokens=20000,\n    output_mode=\"multi_hot\",\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-09T17:49:06.388859Z","iopub.execute_input":"2023-08-09T17:49:06.389619Z","iopub.status.idle":"2023-08-09T17:49:06.400592Z","shell.execute_reply.started":"2023-08-09T17:49:06.389585Z","shell.execute_reply":"2023-08-09T17:49:06.399381Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"### Training and testing the binary bigram model","metadata":{}},{"cell_type":"code","source":"text_vectorization.adapt(text_only_train_ds)\nbinary_2gram_train_ds = train_ds.map(\n    lambda x, y: (text_vectorization(x), y),\n    num_parallel_calls=4)\nbinary_2gram_val_ds = val_ds.map(\n    lambda x, y: (text_vectorization(x), y),\n    num_parallel_calls=4)\nbinary_2gram_test_ds = test_ds.map(\n    lambda x, y: (text_vectorization(x), y),\n    num_parallel_calls=4)\nmodel = get_model()\nmodel.summary()\ncallbacks = [\n    keras.callbacks.ModelCheckpoint(\"binary_2gram.keras\",\n                                    save_best_only=True)\n]\nmodel.fit(binary_2gram_train_ds.cache(),\n        validation_data=binary_2gram_val_ds.cache(),\n        epochs=10,\n        callbacks=callbacks)\nmodel = keras.models.load_model(\"binary_2gram.keras\")\nprint(f\"Test acc: {model.evaluate(binary_2gram_test_ds)[1]:.3f}\")","metadata":{"execution":{"iopub.status.busy":"2023-08-09T17:49:09.524661Z","iopub.execute_input":"2023-08-09T17:49:09.525066Z","iopub.status.idle":"2023-08-09T17:49:58.563243Z","shell.execute_reply.started":"2023-08-09T17:49:09.525035Z","shell.execute_reply":"2023-08-09T17:49:58.561940Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Model: \"model_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_2 (InputLayer)        [(None, 20000)]           0         \n                                                                 \n dense_2 (Dense)             (None, 16)                320016    \n                                                                 \n dropout_1 (Dropout)         (None, 16)                0         \n                                                                 \n dense_3 (Dense)             (None, 1)                 17        \n                                                                 \n=================================================================\nTotal params: 320,033\nTrainable params: 320,033\nNon-trainable params: 0\n_________________________________________________________________\nEpoch 1/10\n625/625 [==============================] - 7s 10ms/step - loss: 0.3756 - accuracy: 0.8429 - val_loss: 0.2694 - val_accuracy: 0.8888\nEpoch 2/10\n625/625 [==============================] - 3s 4ms/step - loss: 0.2406 - accuracy: 0.9143 - val_loss: 0.2763 - val_accuracy: 0.8936\nEpoch 3/10\n625/625 [==============================] - 3s 4ms/step - loss: 0.2004 - accuracy: 0.9340 - val_loss: 0.2949 - val_accuracy: 0.8914\nEpoch 4/10\n625/625 [==============================] - 3s 4ms/step - loss: 0.1877 - accuracy: 0.9424 - val_loss: 0.3128 - val_accuracy: 0.8888\nEpoch 5/10\n625/625 [==============================] - 3s 4ms/step - loss: 0.1775 - accuracy: 0.9477 - val_loss: 0.3274 - val_accuracy: 0.8892\nEpoch 6/10\n625/625 [==============================] - 3s 4ms/step - loss: 0.1708 - accuracy: 0.9514 - val_loss: 0.3445 - val_accuracy: 0.8914\nEpoch 7/10\n625/625 [==============================] - 3s 4ms/step - loss: 0.1662 - accuracy: 0.9540 - val_loss: 0.3642 - val_accuracy: 0.8892\nEpoch 8/10\n625/625 [==============================] - 3s 4ms/step - loss: 0.1603 - accuracy: 0.9569 - val_loss: 0.3731 - val_accuracy: 0.8808\nEpoch 9/10\n625/625 [==============================] - 3s 4ms/step - loss: 0.1623 - accuracy: 0.9574 - val_loss: 0.4002 - val_accuracy: 0.8774\nEpoch 10/10\n625/625 [==============================] - 3s 4ms/step - loss: 0.1586 - accuracy: 0.9583 - val_loss: 0.3980 - val_accuracy: 0.8794\n782/782 [==============================] - 5s 6ms/step - loss: 0.2679 - accuracy: 0.8966\nTest acc: 0.897\n","output_type":"stream"}]},{"cell_type":"markdown","source":"We’re now getting 90% test accuracy, a marked improvement! Turns out local order\nis pretty important. ","metadata":{}},{"cell_type":"markdown","source":"**BIGRAMS WITH TF-IDF ENCODING**\n\nYou can also add a bit more information to this representation by counting how many\ntimes each word or N-gram occurs, that is to say, by taking the histogram of the words\nover the text:\n```\n{\"the\": 2, \"the cat\": 1, \"cat\": 1, \"cat sat\": 1, \"sat\": 1,\n \"sat on\": 1, \"on\": 1, \"on the\": 1, \"the mat: 1\", \"mat\": 1}\n```\nIf you’re doing text classification, knowing how many times a word occurs in a sample\nis critical: any sufficiently long movie review may contain the word “terrible” regardless of sentiment, but a review that contains many instances of the word “terrible” is\nlikely a negative one.\n Here’s how you’d count bigram occurrences with the TextVectorization layer.\n \n###  Configuring the TextVectorization layer to return token counts","metadata":{}},{"cell_type":"code","source":"text_vectorization = TextVectorization(\n    ngrams=2,\n    max_tokens=20000,\n    output_mode=\"count\"\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-09T17:50:09.285326Z","iopub.execute_input":"2023-08-09T17:50:09.285765Z","iopub.status.idle":"2023-08-09T17:50:09.297026Z","shell.execute_reply.started":"2023-08-09T17:50:09.285734Z","shell.execute_reply":"2023-08-09T17:50:09.295964Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"Now, of course, some words are bound to occur more often than others no matter\nwhat the text is about. The words “the,” “a,” “is,” and “are” will always dominate your\nword count histograms, drowning out other words—despite being pretty much useless\nfeatures in a classification context. How could we address this?\n\n You already guessed it: via normalization. We could just normalize word counts by\nsubtracting the mean and dividing by the variance (computed across the entire training dataset). That would make sense. Except most vectorized sentences consist almost\nentirely of zeros (our previous example features 12 non-zero entries and 19,988 zero\nentries), a property called “sparsity.” That’s a great property to have, as it dramatically\nreduces compute load and reduces the risk of overfitting. If we subtracted the mean\nfrom each feature, we’d wreck sparsity. Thus, whatever normalization scheme we use\nshould be divide-only. What, then, should we use as the denominator? The best practice is to go with something called *TF-IDF normalization*—TF-IDF stands for “term frequency, inverse document frequency.”\n\n TF-IDF is so common that it’s built into the `TextVectorization` layer. All you need\nto do to start using it is to switch the `output_mode` argument to `\"tf_idf\"`.","metadata":{}},{"cell_type":"markdown","source":"**Understanding TF-IDF normalization**\n\nThe more a given term appears in a document, the more important that term is for\nunderstanding what the document is about. At the same time, the frequency at which\nthe term appears across all documents in your dataset matters too: terms that\nappear in almost every document (like “the” or “a”) aren’t particularly informative,\nwhile terms that appear only in a small subset of all texts (like “Herzog”) are very distinctive, and thus important. TF-IDF is a metric that fuses these two ideas. It weights\na given term by taking “term frequency,” how many times the term appears in the\ncurrent document, and dividing it by a measure of “document frequency,” which estimates how often the term comes up across the dataset. You’d compute it as follows:\n``` python\ndef tfidf(term, document, dataset):\n    term_freq = document.count(term)\n    doc_freq = math.log(sum(doc.count(term) for doc in dataset) + 1)\n    return term_freq / doc_freq\n```","metadata":{}},{"cell_type":"markdown","source":"### Configuring `TextVectorization` to return TF-IDF-weighted outputs","metadata":{}},{"cell_type":"code","source":"text_vectorization = TextVectorization(\n    ngrams=2,\n    max_tokens=20000,\n    output_mode=\"tf_idf\",\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-09T17:50:15.851314Z","iopub.execute_input":"2023-08-09T17:50:15.851684Z","iopub.status.idle":"2023-08-09T17:50:15.868719Z","shell.execute_reply.started":"2023-08-09T17:50:15.851653Z","shell.execute_reply":"2023-08-09T17:50:15.867762Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"### Training and testing the TF-IDF bigram model","metadata":{}},{"cell_type":"code","source":"# The adapt() call will learn the TF-IDF weights in addition to the vocabulary.\ntext_vectorization.adapt(text_only_train_ds)\n\ntfidf_2gram_train_ds = train_ds.map(\n    lambda x, y: (text_vectorization(x), y),\n    num_parallel_calls=4)\ntfidf_2gram_val_ds = val_ds.map(\n    lambda x, y: (text_vectorization(x), y),\n    num_parallel_calls=4)\ntfidf_2gram_test_ds = test_ds.map(\n    lambda x, y: (text_vectorization(x), y),\n    num_parallel_calls=4)\n\nmodel = get_model()\nmodel.summary()\ncallbacks = [\n    keras.callbacks.ModelCheckpoint(\"tfidf_2gram.keras\",\n                                    save_best_only=True)\n]\nmodel.fit(tfidf_2gram_train_ds.cache(),\n        validation_data=tfidf_2gram_val_ds.cache(),\n        epochs=10,\n        callbacks=callbacks)\nmodel = keras.models.load_model(\"tfidf_2gram.keras\")\nprint(f\"Test acc: {model.evaluate(tfidf_2gram_test_ds)[1]:.3f}\")","metadata":{"execution":{"iopub.status.busy":"2023-08-09T17:50:19.513323Z","iopub.execute_input":"2023-08-09T17:50:19.513692Z","iopub.status.idle":"2023-08-09T17:51:24.359585Z","shell.execute_reply.started":"2023-08-09T17:50:19.513662Z","shell.execute_reply":"2023-08-09T17:51:24.358570Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Model: \"model_2\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_3 (InputLayer)        [(None, 20000)]           0         \n                                                                 \n dense_4 (Dense)             (None, 16)                320016    \n                                                                 \n dropout_2 (Dropout)         (None, 16)                0         \n                                                                 \n dense_5 (Dense)             (None, 1)                 17        \n                                                                 \n=================================================================\nTotal params: 320,033\nTrainable params: 320,033\nNon-trainable params: 0\n_________________________________________________________________\nEpoch 1/10\n625/625 [==============================] - 7s 10ms/step - loss: 0.5089 - accuracy: 0.7667 - val_loss: 0.3366 - val_accuracy: 0.8534\nEpoch 2/10\n625/625 [==============================] - 3s 4ms/step - loss: 0.3500 - accuracy: 0.8597 - val_loss: 0.3147 - val_accuracy: 0.8648\nEpoch 3/10\n625/625 [==============================] - 3s 4ms/step - loss: 0.3010 - accuracy: 0.8815 - val_loss: 0.3082 - val_accuracy: 0.8820\nEpoch 4/10\n625/625 [==============================] - 3s 4ms/step - loss: 0.2725 - accuracy: 0.8909 - val_loss: 0.3315 - val_accuracy: 0.8640\nEpoch 5/10\n625/625 [==============================] - 3s 4ms/step - loss: 0.2742 - accuracy: 0.8910 - val_loss: 0.3347 - val_accuracy: 0.8768\nEpoch 6/10\n625/625 [==============================] - 3s 4ms/step - loss: 0.2564 - accuracy: 0.8956 - val_loss: 0.3555 - val_accuracy: 0.8640\nEpoch 7/10\n625/625 [==============================] - 3s 4ms/step - loss: 0.2523 - accuracy: 0.9005 - val_loss: 0.3584 - val_accuracy: 0.8700\nEpoch 8/10\n625/625 [==============================] - 3s 4ms/step - loss: 0.2395 - accuracy: 0.9044 - val_loss: 0.3819 - val_accuracy: 0.8636\nEpoch 9/10\n625/625 [==============================] - 3s 4ms/step - loss: 0.2397 - accuracy: 0.9073 - val_loss: 0.3838 - val_accuracy: 0.8702\nEpoch 10/10\n625/625 [==============================] - 3s 4ms/step - loss: 0.2288 - accuracy: 0.9132 - val_loss: 0.3838 - val_accuracy: 0.8730\n782/782 [==============================] - 6s 7ms/step - loss: 0.3045 - accuracy: 0.8839\nTest acc: 0.884\n","output_type":"stream"}]},{"cell_type":"markdown","source":"This gets us an 88.6% test accuracy on the IMDB classification task: it doesn’t seem to\nbe particularly helpful in this case. However, for many text-classification datasets, it\nwould be typical to see a one-percentage-point increase when using TF-IDF compared\nto plain binary encoding.","metadata":{}},{"cell_type":"markdown","source":"**Exporting a model that processes raw strings**\n\nIn the preceding examples, we did our text standardization, splitting, and indexing as\npart of the tf.data pipeline. But if we want to export a standalone model independent of this pipeline, we should make sure that it incorporates its own text preprocessing (otherwise, you’d have to reimplement in the production environment, which\ncan be challenging or can lead to subtle discrepancies between the training data and\nthe production data). Thankfully, this is easy.\nJust create a new model that reuses your TextVectorization layer and adds to it\nthe model you just trained:","metadata":{}},{"cell_type":"code","source":"# One input sample would be one string.\ninputs = keras.Input(shape=(1,), dtype=\"string\")\n# Apply text preprocessing.\nprocessed_inputs = text_vectorization(inputs)\n# Apply the previously trained model.\noutputs = model(processed_inputs)\n# Instantiate the end-to-end model.\ninference_model = keras.Model(inputs, outputs)","metadata":{"execution":{"iopub.status.busy":"2023-08-09T17:52:20.378156Z","iopub.execute_input":"2023-08-09T17:52:20.379139Z","iopub.status.idle":"2023-08-09T17:52:20.441559Z","shell.execute_reply.started":"2023-08-09T17:52:20.379089Z","shell.execute_reply":"2023-08-09T17:52:20.440604Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# The resulting model can process batches of raw strings:\nimport tensorflow as tf\nraw_text_data = tf.convert_to_tensor([\n [\"That was an excellent movie, I loved it.\"],\n])\npredictions = inference_model(raw_text_data)\nprint(f\"{float(predictions[0] * 100):.2f} percent positive\")","metadata":{"execution":{"iopub.status.busy":"2023-08-09T17:52:23.945421Z","iopub.execute_input":"2023-08-09T17:52:23.946012Z","iopub.status.idle":"2023-08-09T17:52:24.051603Z","shell.execute_reply.started":"2023-08-09T17:52:23.945967Z","shell.execute_reply":"2023-08-09T17:52:24.050658Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"93.32 percent positive\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 11.3.3 Processing words as a sequence: The sequence model approach\n\nThese past few examples clearly show that word order matters: manual engineering of\norder-based features, such as bigrams, yields a nice accuracy boost. Now remember: the\nhistory of deep learning is that of a move away from manual feature engineering, toward\nletting models learn their own features from exposure to data alone. What if, instead of\nmanually crafting order-based features, we exposed the model to raw word sequences\nand let it figure out such features on its own? This is what *sequence models* are about.\n\nTo implement a sequence model, you’d start by representing your input samples as\nsequences of integer indices (one integer standing for one word). Then, you’d map\neach integer to a vector to obtain vector sequences. Finally, you’d feed these\nsequences of vectors into a stack of layers that could cross-correlate features from adjacent vectors, such as a 1D convnet, a RNN, or a Transformer.\n\n For some time around 2016–2017, bidirectional RNNs (in particular, bidirectional\nLSTMs) were considered to be the state of the art for sequence modeling. However, nowadays sequence modeling is almost universally done with Transformers, which we will cover shortly. Oddly, one-dimensional convnets were never\nvery popular in NLP, even though, in my own experience, a residual stack of depthwise-separable 1D convolutions can often achieve comparable performance to a bidirectional LSTM, at a greatly reduced computational cost.","metadata":{}},{"cell_type":"markdown","source":"**A FIRST PRACTICAL EXAMPLE**\n\nLet’s try out a first sequence model in practice. First, let’s prepare datasets that return\ninteger sequences.\n\n### Preparing integer sequence datasets\n\nIn order to keep a manageable\ninput size, we’ll truncate the\ninputs after the first 600 words.\nThis is a reasonable choice, since\nthe average review length is 233\nwords, and only 5% of reviews\nare longer than 600 words.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras import layers\n\nmax_length = 600\nmax_tokens = 20000\ntext_vectorization = layers.TextVectorization(\n    max_tokens=max_tokens,\n    output_mode=\"int\",\n    output_sequence_length=max_length,\n)\ntext_vectorization.adapt(text_only_train_ds)\n\nint_train_ds = train_ds.map(\n    lambda x, y: (text_vectorization(x), y),\n    num_parallel_calls=4)\nint_val_ds = val_ds.map(\n    lambda x, y: (text_vectorization(x), y),\n    num_parallel_calls=4)\nint_test_ds = test_ds.map(\n    lambda x, y: (text_vectorization(x), y),\n    num_parallel_calls=4)","metadata":{"execution":{"iopub.status.busy":"2023-08-09T17:52:36.537421Z","iopub.execute_input":"2023-08-09T17:52:36.537786Z","iopub.status.idle":"2023-08-09T17:52:39.867647Z","shell.execute_reply.started":"2023-08-09T17:52:36.537754Z","shell.execute_reply":"2023-08-09T17:52:39.866651Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"Next, let’s make a model. The simplest way to convert our integer sequences to vector\nsequences is to one-hot encode the integers (each dimension would represent one\npossible term in the vocabulary). On top of these one-hot vectors, we’ll add a simple\nbidirectional LSTM.\n\n### A sequence model built on one-hot encoded vector sequences","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.compat.v1.keras.layers import CuDNNLSTM\n\n# One input is a sequence of integers.\ninputs = keras.Input(shape=(None,), dtype=\"int64\")\n# Encode the integers into binary 20,000-dimensional vectors.\nembedded = tf.one_hot(inputs, depth=max_tokens)\n# Add a bidirectional LSTM.\nx = layers.Bidirectional(CuDNNLSTM(32))(embedded)\nx = layers.Dropout(0.5)(x) \n# Finally, add a classification layer.\noutputs = layers.Dense(1, activation=\"sigmoid\")(x) \nmodel = keras.Model(inputs, outputs)\nmodel.compile(optimizer=\"rmsprop\",\n             loss=\"binary_crossentropy\",\n             metrics=[\"accuracy\"])\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-08-09T17:52:44.876720Z","iopub.execute_input":"2023-08-09T17:52:44.877101Z","iopub.status.idle":"2023-08-09T17:52:45.617538Z","shell.execute_reply.started":"2023-08-09T17:52:44.877070Z","shell.execute_reply":"2023-08-09T17:52:45.616800Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Model: \"model_4\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_5 (InputLayer)        [(None, None)]            0         \n                                                                 \n tf.one_hot (TFOpLambda)     (None, None, 20000)       0         \n                                                                 \n bidirectional (Bidirectiona  (None, 64)               5128704   \n l)                                                              \n                                                                 \n dropout_3 (Dropout)         (None, 64)                0         \n                                                                 \n dense_6 (Dense)             (None, 1)                 65        \n                                                                 \n=================================================================\nTotal params: 5,128,769\nTrainable params: 5,128,769\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Training a first basic sequence model","metadata":{}},{"cell_type":"code","source":"callbacks = [\n    keras.callbacks.ModelCheckpoint(\"one_hot_bidir_lstm.keras\",\n                                    save_best_only=True)\n]\nmodel.fit(int_train_ds, validation_data=int_val_ds, epochs=10,\n         callbacks=callbacks)\nmodel = keras.models.load_model(\"one_hot_bidir_lstm.keras\")\nprint(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")","metadata":{"execution":{"iopub.status.busy":"2023-08-09T17:53:04.626258Z","iopub.execute_input":"2023-08-09T17:53:04.626647Z","iopub.status.idle":"2023-08-09T18:18:52.192306Z","shell.execute_reply.started":"2023-08-09T17:53:04.626617Z","shell.execute_reply":"2023-08-09T18:18:52.191039Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Epoch 1/10\n625/625 [==============================] - 143s 224ms/step - loss: 0.6056 - accuracy: 0.6693 - val_loss: 0.5056 - val_accuracy: 0.8000\nEpoch 2/10\n625/625 [==============================] - 140s 223ms/step - loss: 0.4159 - accuracy: 0.8308 - val_loss: 0.3253 - val_accuracy: 0.8730\nEpoch 3/10\n625/625 [==============================] - 140s 223ms/step - loss: 0.3322 - accuracy: 0.8795 - val_loss: 0.2900 - val_accuracy: 0.8842\nEpoch 4/10\n625/625 [==============================] - 136s 218ms/step - loss: 0.2926 - accuracy: 0.8932 - val_loss: 0.3244 - val_accuracy: 0.8600\nEpoch 5/10\n625/625 [==============================] - 140s 224ms/step - loss: 0.2539 - accuracy: 0.9140 - val_loss: 0.3027 - val_accuracy: 0.8770\nEpoch 6/10\n625/625 [==============================] - 140s 224ms/step - loss: 0.2355 - accuracy: 0.9191 - val_loss: 0.3407 - val_accuracy: 0.8850\nEpoch 7/10\n625/625 [==============================] - 140s 224ms/step - loss: 0.2094 - accuracy: 0.9298 - val_loss: 0.3609 - val_accuracy: 0.8586\nEpoch 8/10\n625/625 [==============================] - 136s 218ms/step - loss: 0.1947 - accuracy: 0.9366 - val_loss: 0.3350 - val_accuracy: 0.8894\nEpoch 9/10\n625/625 [==============================] - 140s 224ms/step - loss: 0.1760 - accuracy: 0.9424 - val_loss: 0.4731 - val_accuracy: 0.8736\nEpoch 10/10\n625/625 [==============================] - 140s 224ms/step - loss: 0.1597 - accuracy: 0.9488 - val_loss: 0.4048 - val_accuracy: 0.8836\n782/782 [==============================] - 87s 111ms/step - loss: 0.2993 - accuracy: 0.8814\nTest acc: 0.881\n","output_type":"stream"}]},{"cell_type":"markdown","source":"A first observation: this model trains very slowly (for roughly a day if not using the CuDNN version), especially compared to the lightweight model of the previous section. This is because our inputs are quite large: each\ninput sample is encoded as a matrix of size `(600, 20000)` (600 words per sample,\n20,000 possible words). That’s 12,000,000 floats for a single movie review. Our bidirectional LSTM has a lot of work to do. Second, the model only gets to 88% test accuracy—it doesn’t perform nearly as well as our (very fast) binary unigram model.\n Clearly, using one-hot encoding to turn words into vectors, which was the simplest\nthing we could do, wasn’t a great idea. There’s a better way: *word embeddings*.","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}